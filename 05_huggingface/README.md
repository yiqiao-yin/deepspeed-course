# HuggingFace + DeepSpeed Fine-tuning

This guide walks through how to use **DeepSpeed** with **HuggingFace Transformers** to fine-tune large language models efficiently on multi-GPU setups.

## Prerequisites

- Docker image: `runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04` (or similar)
- `uv` package manager installed
- At least 2 GPUs recommended (see [HARDWARE_REQUIREMENTS.md](HARDWARE_REQUIREMENTS.md))
- HuggingFace account with API token (optional, for model download and upload)
- Weights & Biases account with API key (optional, for experiment tracking) 

## Project Starter

Use `uv` to start project. 

```bash
uv init project_name
````

If you do not have `uv`, please install it.

```bash
brew install uv
```

Or alternatively, you can use `pip`.

```bash
pip install uv
```

Next, add packages or dependencies

```bash
cd project_name
uv add torch transformers accelerate datasets deepspeed bitsandbytes trl unsloth wandb
```

Or add them individually:

```bash
uv add torch
uv add transformers
uv add accelerate
uv add datasets
uv add deepspeed
uv add bitsandbytes
uv add trl
uv add unsloth
uv add wandb  # Optional, for experiment tracking
```

We can examine the package dependency trees.

```bash
uv tree
```

You should expect something like the following.

```bash
root@1b0c67c74d6a:/workspace/deepspeed_project# uv tree
Resolved 97 packages in 0.68ms
deepspeed-project v0.1.0
â”œâ”€â”€ accelerate v1.6.0
â”‚   â”œâ”€â”€ huggingface-hub v0.31.1
â”‚   â”‚   â”œâ”€â”€ filelock v3.18.0
â”‚   â”‚   â”œâ”€â”€ fsspec v2025.3.0
â”‚   â”‚   â”‚   â””â”€â”€ aiohttp v3.11.18 (extra: http)
â”‚   â”‚   â”‚       â”œâ”€â”€ aiohappyeyeballs v2.6.1
â”‚   â”‚   â”‚       â”œâ”€â”€ aiosignal v1.3.2
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ frozenlist v1.6.0
â”‚   â”‚   â”‚       â”œâ”€â”€ async-timeout v5.0.1
â”‚   â”‚   â”‚       â”œâ”€â”€ attrs v25.3.0
â”‚   â”‚   â”‚       â”œâ”€â”€ frozenlist v1.6.0
â”‚   â”‚   â”‚       â”œâ”€â”€ multidict v6.4.3
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ typing-extensions v4.13.2
â”‚   â”‚   â”‚       â”œâ”€â”€ propcache v0.3.1
â”‚   â”‚   â”‚       â””â”€â”€ yarl v1.20.0
â”‚   â”‚   â”‚           â”œâ”€â”€ idna v3.10
â”‚   â”‚   â”‚           â”œâ”€â”€ multidict v6.4.3 (*)
â”‚   â”‚   â”‚           â””â”€â”€ propcache v0.3.1
â”‚   â”‚   â”œâ”€â”€ hf-xet v1.1.0
â”‚   â”‚   â”œâ”€â”€ packaging v25.0
â”‚   â”‚   â”œâ”€â”€ pyyaml v6.0.2
â”‚   â”‚   â”œâ”€â”€ requests v2.32.3
â”‚   â”‚   â”‚   â”œâ”€â”€ certifi v2025.4.26
â”‚   â”‚   â”‚   â”œâ”€â”€ charset-normalizer v3.4.2
â”‚   â”‚   â”‚   â”œâ”€â”€ idna v3.10
â”‚   â”‚   â”‚   â””â”€â”€ urllib3 v2.4.0
â”‚   â”‚   â”œâ”€â”€ tqdm v4.67.1
â”‚   â”‚   â””â”€â”€ typing-extensions v4.13.2
â”‚   â”œâ”€â”€ numpy v2.2.5
â”‚   â”œâ”€â”€ packaging v25.0
â”‚   â”œâ”€â”€ psutil v7.0.0
â”‚   â”œâ”€â”€ pyyaml v6.0.2
â”‚   â”œâ”€â”€ safetensors v0.5.3
â”‚   â””â”€â”€ torch v2.7.0
â”‚       â”œâ”€â”€ filelock v3.18.0
â”‚       â”œâ”€â”€ fsspec v2025.3.0 (*)
â”‚       â”œâ”€â”€ jinja2 v3.1.6
â”‚       â”‚   â””â”€â”€ markupsafe v3.0.2
â”‚       â”œâ”€â”€ networkx v3.4.2
â”‚       â”œâ”€â”€ nvidia-cublas-cu12 v12.6.4.1
â”‚       â”œâ”€â”€ nvidia-cuda-cupti-cu12 v12.6.80
â”‚       â”œâ”€â”€ nvidia-cuda-nvrtc-cu12 v12.6.77
â”‚       â”œâ”€â”€ nvidia-cuda-runtime-cu12 v12.6.77
â”‚       â”œâ”€â”€ nvidia-cudnn-cu12 v9.5.1.17
â”‚       â”‚   â””â”€â”€ nvidia-cublas-cu12 v12.6.4.1
â”‚       â”œâ”€â”€ nvidia-cufft-cu12 v11.3.0.4
â”‚       â”‚   â””â”€â”€ nvidia-nvjitlink-cu12 v12.6.85
â”‚       â”œâ”€â”€ nvidia-cufile-cu12 v1.11.1.6
â”‚       â”œâ”€â”€ nvidia-curand-cu12 v10.3.7.77
â”‚       â”œâ”€â”€ nvidia-cusolver-cu12 v11.7.1.2
â”‚       â”‚   â”œâ”€â”€ nvidia-cublas-cu12 v12.6.4.1
â”‚       â”‚   â”œâ”€â”€ nvidia-cusparse-cu12 v12.5.4.2
â”‚       â”‚   â”‚   â””â”€â”€ nvidia-nvjitlink-cu12 v12.6.85
â”‚       â”‚   â””â”€â”€ nvidia-nvjitlink-cu12 v12.6.85
â”‚       â”œâ”€â”€ nvidia-cusparse-cu12 v12.5.4.2 (*)
â”‚       â”œâ”€â”€ nvidia-cusparselt-cu12 v0.6.3
â”‚       â”œâ”€â”€ nvidia-nccl-cu12 v2.26.2
â”‚       â”œâ”€â”€ nvidia-nvjitlink-cu12 v12.6.85
â”‚       â”œâ”€â”€ nvidia-nvtx-cu12 v12.6.77
â”‚       â”œâ”€â”€ sympy v1.14.0
â”‚       â”‚   â””â”€â”€ mpmath v1.3.0
â”‚       â”œâ”€â”€ triton v3.3.0
â”‚       â”‚   â””â”€â”€ setuptools v80.3.1
â”‚       â””â”€â”€ typing-extensions v4.13.2
â”œâ”€â”€ bitsandbytes v0.45.5
â”‚   â”œâ”€â”€ numpy v2.2.5
â”‚   â””â”€â”€ torch v2.7.0 (*)
â”œâ”€â”€ datasets v3.6.0
â”‚   â”œâ”€â”€ dill v0.3.8
â”‚   â”œâ”€â”€ filelock v3.18.0
â”‚   â”œâ”€â”€ fsspec[http] v2025.3.0 (*)
â”‚   â”œâ”€â”€ huggingface-hub v0.31.1 (*)
â”‚   â”œâ”€â”€ multiprocess v0.70.16
â”‚   â”‚   â””â”€â”€ dill v0.3.8
â”‚   â”œâ”€â”€ numpy v2.2.5
â”‚   â”œâ”€â”€ packaging v25.0
â”‚   â”œâ”€â”€ pandas v2.2.3
â”‚   â”‚   â”œâ”€â”€ numpy v2.2.5
â”‚   â”‚   â”œâ”€â”€ python-dateutil v2.9.0.post0
â”‚   â”‚   â”‚   â””â”€â”€ six v1.17.0
â”‚   â”‚   â”œâ”€â”€ pytz v2025.2
â”‚   â”‚   â””â”€â”€ tzdata v2025.2
â”‚   â”œâ”€â”€ pyarrow v20.0.0
```

Afterwards, you should be able to expect the following folder structure:

```bash
project_name/
â”œâ”€â”€ README.md
â”œâ”€â”€ ds_config.json           # DeepSpeed configuration
â”œâ”€â”€ train_ds.py              # Training script
â”œâ”€â”€ pyproject.toml           # UV project configuration
â”œâ”€â”€ uv.lock                  # UV lock file
â””â”€â”€ results/                 # Training outputs
```

## DeepSpeed Configuration

The `ds_config.json` file controls DeepSpeed optimization settings. The most important parameter is the **ZeRO optimization stage**:

### ZeRO Optimization Stages

**Stage 1** - Optimizer State Partitioning:
- Partitions optimizer states across GPUs
- **Memory savings**: ~4x reduction
- **Recommended for**: Models that fit in GPU memory but optimizer states don't
- **Use case**: Smaller models (1B-7B parameters) on GPUs with limited memory

```json
{
  "zero_optimization": {
    "stage": 1
  }
}
```

**Stage 2** - Optimizer + Gradient Partitioning:
- Partitions both optimizer states AND gradients across GPUs
- **Memory savings**: ~8x reduction
- **Recommended for**: Medium models (7B-13B parameters) or limited GPU memory
- **Use case**: Llama-3.2-3B on 2x RTX 4090 or similar

```json
{
  "zero_optimization": {
    "stage": 2
  }
}
```

**Stage 3** - Optimizer + Gradient + Parameter Partitioning:
- Partitions optimizer states, gradients, AND model parameters across GPUs
- **Memory savings**: Linear with number of GPUs
- **Recommended for**: Very large models (13B+ parameters)
- **Use case**: Large models that don't fit in single GPU memory
- **Note**: Slightly slower due to increased communication

```json
{
  "zero_optimization": {
    "stage": 3
  }
}
```

### Switching ZeRO Stages

To change the ZeRO stage, simply edit `ds_config.json`:

```json
{
  "train_batch_size": 32,
  "gradient_accumulation_steps": 1,
  "fp16": {
    "enabled": false
  },
  "zero_optimization": {
    "stage": 2  # Change this to 1, 2, or 3
  }
}
```

## Environment Setup

Before running the training script, set up your API tokens:

### Required: HuggingFace Token

```bash
export HF_TOKEN="your_huggingface_token_here"
```

Get your token from: https://huggingface.co/settings/tokens

### Optional: Weights & Biases API Key

For experiment tracking and visualization:

```bash
export WANDB_API_KEY="your_wandb_api_key_here"
```

Get your API key from: https://wandb.ai/authorize

If you don't set `WANDB_API_KEY`, the script will run without W&B tracking.

## Run Training

### Option 1: Using DeepSpeed Launcher (Recommended for 2+ GPUs)

For multi-GPU training with 2 GPUs:

```bash
uv run deepspeed --num_gpus=2 train_ds.py
```

For all available GPUs:

```bash
uv run deepspeed --num_gpus=$(nvidia-smi --list-gpus | wc -l) train_ds.py
```

### Option 2: Using Standard Python (Single GPU)

```bash
uv run python train_ds.py
```

### Option 3: Manual DeepSpeed Configuration

With custom DeepSpeed launcher arguments:

```bash
uv run deepspeed \
  --num_gpus=2 \
  --master_port=29500 \
  train_ds.py
```

## Monitoring Training

### Local Monitoring

Watch GPU utilization:

```bash
watch -n 1 nvidia-smi
```

### W&B Dashboard (if enabled)

After starting training with `WANDB_API_KEY` set, you'll see:

```
âœ… Weights & Biases: Enabled
ðŸ“ˆ W&B Run initialized: llama-3.2-3b-warren-buffett
   View at: https://wandb.ai/your-username/huggingface-deepspeed-finetuning/runs/xxxxx
```

Visit the URL to see real-time metrics, including:
- Training loss
- Learning rate
- GPU utilization
- System metrics

## Common Issues

### Out of Memory (OOM)

Try these in order:
1. Reduce `per_device_train_batch_size` in `train_ds.py`
2. Increase `gradient_accumulation_steps` in `ds_config.json`
3. Switch to higher ZeRO stage (1 â†’ 2 â†’ 3)
4. Enable FP16/BF16 mixed precision in `ds_config.json`

### NCCL Errors

If you see NCCL timeout errors:

```bash
export NCCL_P2P_DISABLE=1
export NCCL_IB_DISABLE=1
```

Then rerun the training command.

## Hardware Requirements

See [HARDWARE_REQUIREMENTS.md](HARDWARE_REQUIREMENTS.md) for detailed GPU requirements and recommendations for different models.