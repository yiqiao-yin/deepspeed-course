# Video-Text-to-Text Training with DeepSpeed ğŸ¥ğŸ“

This directory contains **two separate video training frameworks** optimized for different use cases:

1. **LLaVA Video Trainer** - Vision-language understanding with actual video frame processing
2. **Seq2Seq Video Trainer** - Text-to-text generation with video metadata

---

## ğŸ“ Directory Structure

```
08_vtt/hf_ds_vtt_test2/
â”œâ”€â”€ llava_video_trainer/              # LLaVA vision-language trainer
â”‚   â”œâ”€â”€ video_training_script.py      # LLaVA training script (generates config)
â”‚   â”œâ”€â”€ run_training.sh               # SLURM/multi-GPU launcher
â”‚   â””â”€â”€ README.md                      # Comprehensive LLaVA guide
â”‚
â”œâ”€â”€ seq2seq_video_trainer/            # Seq2Seq text-to-text trainer
â”‚   â”œâ”€â”€ video_text_trainer.py         # Seq2Seq training script (uses external config)
â”‚   â”œâ”€â”€ ds_config.json                # External DeepSpeed config file
â”‚   â”œâ”€â”€ run_training.sh               # SLURM/multi-GPU launcher
â”‚   â””â”€â”€ README.md                      # Comprehensive Seq2Seq guide
â”‚
â””â”€â”€ README.md                          # This file
```

---

## ğŸ” Quick Comparison

| Feature | LLaVA Trainer | Seq2Seq Trainer |
|---------|---------------|-----------------|
| **Use Case** | Video understanding | Text generation about videos |
| **Model** | LLaVA (7B) | NLLB (600M) |
| **Architecture** | `LlavaForConditionalGeneration` | `AutoModelForSeq2SeqLM` |
| **Visual Processing** | âœ… Extracts & processes video frames | âŒ Text-only |
| **Frame Extraction** | âœ… 5 PIL Images per video | âŒ None |
| **Config Generation** | ğŸ”§ Generated internally (`"auto"`) | ğŸ“„ External file (hardcoded) |
| **Config File** | Created by script | Must exist (`ds_config.json`) |
| **Model Size** | ~14GB (FP16) | ~2.4GB (FP16) |
| **GPU Memory** | ~16-20GB per GPU | ~6-8GB per GPU |
| **Disk Monitoring** | âœ… Built-in | âŒ None |
| **Dependencies** | PIL, requests, transformers, trl | transformers, trl only |
| **Training Time** | 10-15 min (4 GPUs, 3 epochs) | 2-3 min (4 GPUs, 3 epochs) |
| **Best For** | Visual video understanding | Lightweight text tasks |

---

## ğŸ¯ Which One Should I Use?

### Use **LLaVA Video Trainer** (`llava_video_trainer/`) when:

âœ… You need **actual video understanding** through visual frames
âœ… Working on **vision-language tasks** (video QA, visual captioning)
âœ… Need **multi-frame video analysis**
âœ… Building **conversation systems** about visual content
âœ… Have **16GB+ GPU memory** available
âœ… Can afford **longer training times**

**Example tasks:**
- "What is the cat doing in this video?" (needs to see frames)
- "Describe the scene visually"
- "Count the objects in this video"
- Multi-turn conversations about video content

### Use **Seq2Seq Video Trainer** (`seq2seq_video_trainer/`) when:

âœ… Working with **text-only** video tasks
âœ… Processing **video metadata** (titles, descriptions, tags)
âœ… Need a **smaller, faster** model
âœ… Have **limited GPU resources** (6-8GB is enough)
âœ… Want **quick iteration cycles**
âœ… Don't need actual visual understanding

**Example tasks:**
- "Generate a description from video title"
- "Translate video captions"
- Text-based question answering
- Metadata refinement

---

## ğŸ”§ DeepSpeed Configuration: Key Difference

### LLaVA Trainer: Auto-Generated Config

```python
# In video_training_script.py
def create_deepspeed_config(config_path="ds_config.json"):
    config = {
        "optimizer": {
            "params": {
                "lr": "auto",          # âœ… Syncs with TrainingArguments
                "betas": "auto",
                "weight_decay": "auto"
            }
        },
        "train_batch_size": "auto",    # âœ… Calculated automatically
        "train_micro_batch_size_per_gpu": "auto",
        # ...
    }
```

**âœ… Advantages:**
- No manual synchronization needed
- Single source of truth (TrainingArguments)
- Less error-prone
- Easier to modify

**ğŸ“ When it's created:** Called in `main()` before training starts

---

### Seq2Seq Trainer: External Config File

```json
// ds_config.json (must exist before running)
{
  "optimizer": {
    "params": {
      "lr": 5e-5,                // âš ï¸ Must match TrainingArguments!
      "weight_decay": 0.01
    }
  },
  "train_batch_size": 4,         // âš ï¸ Must calculate correctly!
  "train_micro_batch_size_per_gpu": 1
}
```

**âš ï¸ Important:**
- Values are hardcoded
- Must manually sync with `TrainingArguments`
- Formula: `train_batch_size = per_device_batch Ã— num_gpus Ã— grad_accum_steps`
- If you change learning rate in TrainingArguments, update in config too!

**ğŸ“ Location:** `seq2seq_video_trainer/ds_config.json` (must exist)

---

## ğŸš€ Quick Start

### LLaVA Video Trainer

```bash
cd llava_video_trainer

# Set environment
export HF_USER_ID=your_username
export HF_TOKEN=your_token

# Run training (config generated automatically)
chmod +x run_training.sh
./run_training.sh 4  # 4 GPUs
```

**Output:**
- Dataset: `{HF_USER_ID}/llava-video-text-dataset`
- Model: `{HF_USER_ID}/llava-video-text-model`
- Config: `ds_config.json` (generated)

---

### Seq2Seq Video Trainer

```bash
cd seq2seq_video_trainer

# Set environment
export HF_USER_ID=your_username
export HF_TOKEN=your_token

# Run training (uses existing ds_config.json)
chmod +x run_training.sh
./run_training.sh 4  # 4 GPUs
```

**Output:**
- Dataset: `{HF_USER_ID}/video-text-dataset`
- Model: `{HF_USER_ID}/video-text-model`
- Config: `ds_config.json` (must exist beforehand)

---

## ğŸ“Š Resource Requirements

### LLaVA Trainer

```
Model: llava-hf/llava-interleave-qwen-7b-hf
â”œâ”€â”€ Parameters: 7 billion
â”œâ”€â”€ Model Size: ~14GB (FP16)
â”œâ”€â”€ GPU Memory: 16-20GB per GPU (ZeRO-2)
â”œâ”€â”€ Recommended GPUs: 2-4x A100/H100 (40GB+)
â”œâ”€â”€ Training Time: 10-15 min (4 GPUs, 3 epochs, 4 samples)
â”œâ”€â”€ Disk Space: Monitor actively (script has checks)
â””â”€â”€ Dependencies: torch, deepspeed, transformers, trl, pillow, requests
```

### Seq2Seq Trainer

```
Model: facebook/nllb-200-distilled-600M
â”œâ”€â”€ Parameters: 600 million
â”œâ”€â”€ Model Size: ~2.4GB (FP16)
â”œâ”€â”€ GPU Memory: 6-8GB per GPU (ZeRO-2)
â”œâ”€â”€ Recommended GPUs: 2-4x RTX 3090/4090 or A40
â”œâ”€â”€ Training Time: 2-3 min (4 GPUs, 3 epochs, 4 samples)
â”œâ”€â”€ Disk Space: Moderate (checkpoints created)
â””â”€â”€ Dependencies: torch, deepspeed, transformers, trl
```

---

## ğŸ“ Training Workflow Comparison

### LLaVA: Vision-Language Pipeline

```
1. Download video â†’ Extract 5 frames â†’ PIL Images
2. Create LLaVA conversation format with <image> tokens
3. Generate DeepSpeed config with "auto" values
4. Train with vision-language model
5. Push directly to Hub (no local checkpoints)
```

### Seq2Seq: Text-Only Pipeline

```
1. Reference video URL â†’ No frame extraction
2. Create simple question-caption pairs
3. Use existing ds_config.json
4. Train with text-to-text model
5. Standard checkpointing + Hub push
```

---

## ğŸ“ Data Format Examples

### LLaVA Format (Vision-Language)

```json
{
  "conversation": [
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "What is in this video?"},
        {"type": "image"},  // Frame 1
        {"type": "image"},  // Frame 2
        {"type": "image"},  // Frame 3
        {"type": "image"},  // Frame 4
        {"type": "image"}   // Frame 5
      ]
    },
    {
      "role": "assistant",
      "content": [{"type": "text", "text": "There is a cat in the video."}]
    }
  ],
  "video_url": "https://example.com/video.mp4",
  "num_frames": 5
}
```

### Seq2Seq Format (Text-Only)

```json
{
  "video": "https://example.com/video.mp4",
  "question": "What is in this video?",
  "caption": "There is a cat in the video."
}
```

---

## ğŸ” Detailed Documentation

Each subdirectory contains a **comprehensive README** with:

- âœ… Detailed architecture explanation
- âœ… Step-by-step setup instructions
- âœ… Config file management guide
- âœ… Training examples and commands
- âœ… Troubleshooting section
- âœ… Resource requirements
- âœ… Example usage after training

**Navigate to:**
- [`llava_video_trainer/README.md`](./llava_video_trainer/README.md) - LLaVA guide
- [`seq2seq_video_trainer/README.md`](./seq2seq_video_trainer/README.md) - Seq2Seq guide

---

## ğŸ› ï¸ Troubleshooting Quick Reference

### LLaVA Trainer Issues

| Issue | Solution |
|-------|----------|
| Out of GPU memory | Reduce `num_frames` from 5 to 3 |
| Disk space errors | Script monitors automatically; clean `/tmp` |
| Processor errors | Script auto-fixes LLaVA processor issues |
| Rate limiting | Built-in exponential backoff retry |

### Seq2Seq Trainer Issues

| Issue | Solution |
|-------|----------|
| Batch size mismatch | Sync `ds_config.json` with `TrainingArguments` |
| Learning rate not applied | Check both config file and TrainingArguments |
| Config not found | Ensure `ds_config.json` exists in directory |
| Out of memory | Reduce batch size in both places |

---

## ğŸ“š References

- [LLaVA Paper](https://arxiv.org/abs/2304.08485)
- [DeepSpeed Documentation](https://www.deepspeed.ai/)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [TRL (Transformer Reinforcement Learning)](https://huggingface.co/docs/trl)

---

## ğŸ¯ Summary

| Aspect | LLaVA | Seq2Seq |
|--------|-------|---------|
| **Config Strategy** | Generated with `"auto"` | External hardcoded file |
| **Visual Processing** | âœ… Real frames | âŒ None |
| **Complexity** | High | Low |
| **GPU Requirements** | 16-20GB | 6-8GB |
| **Use Case** | Video understanding | Text generation |
| **Setup Difficulty** | Easy (auto config) | Moderate (manual sync) |

---

**ğŸ’¡ Bottom Line:** Choose **LLaVA** for visual video understanding tasks and **Seq2Seq** for lightweight text-based video tasks. The key difference is that LLaVA processes actual video frames while Seq2Seq only handles text metadata. Configuration-wise, LLaVA auto-generates config while Seq2Seq uses an external file that must be manually synced.

**ğŸ“– For detailed instructions, see the README.md in each subdirectory!**
