# DeepSpeed Course ğŸš€

**Author:** Yiqiao Yin  
[LinkedIn](https://www.linkedin.com/in/yiqiaoyin/) | [YouTube](https://youtube.com/YiqiaoYin/)

## Situation Today ğŸ¢

Training and inference for deep learning models are often slow and resource-intensive, especially as model sizes and dataset complexity grow. This bottleneck impacts productivity and limits experimentation, making it difficult to iterate quickly or deploy models efficiently.

## Problem Statement ğŸ¤”

To overcome these challenges, it's essential to leverage multiple GPUs and distributed training. DeepSpeed is a deep learning optimization library that enables faster training, efficient memory usage, and scalable distributed training across multiple GPUs. Using DeepSpeed can significantly reduce training time and improve inference speed, making it possible to work with larger models and datasets.

## Solution ğŸ’¡

This repository provides a collection of basic frameworks and examples demonstrating how to use DeepSpeed for distributed training and inference. Each folder contains a different neural network architecture or use case, showing how DeepSpeed can be integrated to accelerate workflows.

### Folder Structure ğŸ“

```
deepspeed-course/
â”œâ”€â”€ 01_basic_neuralnet/
â”‚   â”œâ”€â”€ train_ds.py                    # Basic neural network training
â”‚   â”œâ”€â”€ train_ds_enhanced.py           # Enhanced with W&B tracking
â”‚   â”œâ”€â”€ ds_config.json                 # DeepSpeed configuration
â”‚   â”œâ”€â”€ run_deepspeed.sh              # SLURM batch script
â”‚   â””â”€â”€ README.md                      # Documentation
â”‚
â”œâ”€â”€ 02_basic_convnet/
â”‚   â”œâ”€â”€ train_ds.py                    # CNN training on synthetic MNIST
â”‚   â”œâ”€â”€ ds_config.json                 # DeepSpeed configuration
â”‚   â”œâ”€â”€ run_deepspeed.sh              # SLURM batch script
â”‚   â””â”€â”€ README.md                      # Documentation
â”‚
â”œâ”€â”€ 02_basic_convnet_cifar10_examples/
â”‚   â”œâ”€â”€ cifar10_deepspeed.py          # CIFAR-10 CNN (81% accuracy!)
â”‚   â”œâ”€â”€ ds_config.json                 # DeepSpeed config (SGD + BatchNorm)
â”‚   â”œâ”€â”€ run_deepspeed.sh              # SLURM batch script (2 GPUs)
â”‚   â”œâ”€â”€ MODEL_IMPROVEMENT_STRATEGY.md  # Technical deep dive
â”‚   â””â”€â”€ README.md                      # Comprehensive guide
â”‚
â”œâ”€â”€ 03_basic_rnn/
â”‚   â”œâ”€â”€ train_rnn_deepspeed.py        # LSTM time series prediction
â”‚   â”œâ”€â”€ ds_config_rnn.json            # DeepSpeed config (ZeRO-2 + FP16)
â”‚   â”œâ”€â”€ run_deepspeed.sh              # SLURM batch script
â”‚   â””â”€â”€ README.md                      # Documentation with W&B guide
â”‚
â”œâ”€â”€ 04_transferlearning/               # (TBD)
â”œâ”€â”€ 05_huggingface/                    # HuggingFace examples
â”œâ”€â”€ 05_huggingface_trl/                # TRL (Transformer Reinforcement Learning)
â”œâ”€â”€ 06_huggingface_grpo/               # GRPO (Group Relative Policy Optimization)
â”œâ”€â”€ 07_huggingface_openai_gpt_oss_finetune_sft/  # SFT examples
â”œâ”€â”€ 07_huggingface_trl_multi_agency/   # Multi-agent systems
â”œâ”€â”€ 08_vtt/                            # Vision-Text-Text models
â””â”€â”€ README.md                          # This file
```

### Quick Start with SLURM Batch Jobs ğŸš€

Each training folder (01-03) includes a `run_deepspeed.sh` SLURM batch script for running on HPC clusters like CoreWeave. To use:

```bash
# 1. Navigate to desired folder
cd 02_basic_convnet_cifar10_examples

# 2. Edit the SLURM script to configure:
#    - WANDB_API_KEY (get from https://wandb.ai/authorize)
#    - Virtual environment path
nano run_deepspeed.sh

# 3. Submit to SLURM queue
sbatch run_deepspeed.sh

# 4. Monitor job status
squeue -u $USER

# 5. Check logs
tail -f logs/cifar10_cnn_<job_id>.out
```

**Script Features:**
- âœ… Pre-configured GPU/CPU/memory resources per workload
- âœ… Automatic log directory creation
- âœ… Job information printing (ID, node, GPUs, timestamps)
- âœ… W&B API key integration with placeholder
- âœ… Optimized for CoreWeave/SLURM clusters

Explore each folder for step-by-step guides and code samples to accelerate your deep learning projects with DeepSpeed!

### On Runpod

For language models or vision-language models, it is recommended to use the Runpod image: **Runpod Pytorch 2.8.0**

`runpod/pytorch:2.8.0-py3.11-cuda12.8.1-cudnn-devel-ubuntu22.04`

**Pricing Summary**
- GPU Cost: $30.32 / hr
- Running Pod Disk Cost: $0.011 / hr
- Stopped Pod Disk Cost: $0.014 / hr

**Pod Summary**
- 8x H200 SXM (1128 GB VRAM)
- 2008 GB RAM â€¢ 224 vCPU
- Total Disk: 80 GB

For single GPU usage with long training times, it is recommended to use the following Runpod configuration:

**Pricing Summary**
- GPU Cost: $4 / hr
- Running Pod Disk Cost: $0.011 / hr
- Stopped Pod Disk Cost: $0.014 / hr

**Pod Summary**
- 10x A40 (480 GB VRAM)
- 500 GB RAM â€¢ 90 vCPU
- Total Disk: 80 GB