{"cells":[{"cell_type":"code","execution_count":null,"id":"cde807c7-cef2-4fe9-9c6d-9c2f22cc4549","metadata":{"id":"cde807c7-cef2-4fe9-9c6d-9c2f22cc4549","outputId":"5f5cd0e9-3f5e-43e0-c4ad-7c7adad716d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","=== Generated Response ===\n","<|user|>\n","Set a timer for 5 minutes.\n","<|assistant|>\n","Okay, the user wants me to set a timer for 5 minutes. Let me think about how to approach this.\n","\n","First, I need to figure out what exactly the user is asking for. They mentioned a timer, so they probably want a device to set a 5-minute timer. But since I can't use external devices, I should simulate that.\n","\n","I should create a simple script or a system that can set a 5-minute timer. Let's consider using a simple timer function in a\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","# Load base model and tokenizer\n","model_name = \"Qwen/Qwen3-0.6B\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n","\n","# Ensure model is on the right device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)\n","\n","# Sample user message\n","sample_messages = [\n","    {\"role\": \"user\", \"content\": \"Set a timer for 5 minutes.\"}\n","]\n","\n","# Convert message format to prompt\n","def format_messages(messages):\n","    prompt = \"\"\n","    for msg in messages:\n","        role = msg[\"role\"]\n","        content = msg.get(\"content\", \"\")\n","        if role == \"user\":\n","            prompt += f\"<|user|>\\n{content}\\n\"\n","        elif role == \"assistant\":\n","            prompt += f\"<|assistant|>\\n{content}\\n\"\n","        elif role == \"tool\":\n","            prompt += f\"<|tool|>\\n{msg['name']}: {content}\\n\"\n","    prompt += \"<|assistant|>\\n\"  # Let model continue as assistant\n","    return prompt\n","\n","# Format and tokenize\n","formatted_prompt = format_messages(sample_messages)\n","inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n","\n","# Generate response\n","with torch.no_grad():\n","    outputs = model.generate(**inputs, max_new_tokens=100)\n","\n","# Decode and print\n","response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(\"\\n=== Generated Response ===\")\n","print(response)\n"]},{"cell_type":"code","execution_count":null,"id":"7368c51c-c9c3-4b88-8159-b3c7fd722c39","metadata":{"id":"7368c51c-c9c3-4b88-8159-b3c7fd722c39"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}