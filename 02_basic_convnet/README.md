# Enhanced Convolutional Neural Network with DeepSpeed

Train an enhanced CNN for image classification using DeepSpeed on synthetic MNIST-like data with production-ready training features.

## Features

- üñºÔ∏è **Image Classification**: Enhanced CNN architecture for 28x28 grayscale image classification
- üéØ **MNIST-Style Task**: 10-class classification on synthetic data
- ‚ö° **DeepSpeed Integration**: Distributed training with DeepSpeed optimization
- üîß **FP16 Training**: Mixed precision training for faster computation
- üíª **Multi-GPU Ready**: Supports distributed training across multiple GPUs
- üìä **Modern Architecture**: Conv layers + Pooling + Fully Connected layers
- üéì **Kaiming Initialization**: Better weight initialization for ReLU networks
- üìà **Learning Rate Scheduling**: Warmup + Cosine decay for optimal convergence
- üõë **Early Stopping**: Patience-based early stopping to prevent overfitting
- üìâ **Gradient Monitoring**: Track gradient norms throughout training
- üéØ **Accuracy Tracking**: Real-time training accuracy calculation and logging
- üîç **Loss Plateau Detection**: Automatic detection of training plateaus
- üìä **W&B Integration**: Optional Weights & Biases tracking and visualization
- üèÜ **Quality Assessment**: Automatic model performance evaluation

## Quick Start on RunPod

### 1. Initial Setup

Start with a fresh RunPod instance (recommend >= 1x RTX 4090 or A100):

```bash
# Install uv package manager
pip install uv

# Initialize new project
uv init basic-convnet-ds
cd basic-convnet-ds

# Add core dependencies
uv add "torch>=2.0.0"
uv add "deepspeed>=0.12.0"

# Optional: Add Weights & Biases for tracking
uv add "wandb"

# Development dependencies
uv add --dev "black" "isort" "flake8"
```

### 2. Project Structure

Create the following directory structure:

```
basic-convnet-ds/
‚îú‚îÄ‚îÄ train_ds.py            # Your enhanced training script
‚îú‚îÄ‚îÄ ds_config.json         # DeepSpeed configuration
‚îú‚îÄ‚îÄ requirements.txt       # Generated by uv
‚îî‚îÄ‚îÄ README.md             # This file
```

### 3. DeepSpeed Configuration

Create `ds_config.json`:

```json
{
  "train_batch_size": 32,
  "train_micro_batch_size_per_gpu": 32,
  "gradient_accumulation_steps": 1,
  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": 1e-3
    }
  },
  "fp16": {
    "enabled": true
  }
}
```

### 4. Add Your Training Script

Copy your `train_ds.py` script to the project directory.

### 5. (Optional) Configure Weights & Biases

To enable experiment tracking with W&B:

```bash
# Set your W&B API key
export WANDB_API_KEY=your_api_key_here

# Or configure it interactively
wandb login
```

## Running the Training

### Single GPU Training

```bash
uv run deepspeed --num_gpus=1 train_ds.py
```

### Multi-GPU Training with DeepSpeed

```bash
# For 2 GPUs
uv run deepspeed --num_gpus=2 train_ds.py

# For 4 GPUs
uv run deepspeed --num_gpus=4 train_ds.py

# For multi-node training
uv run deepspeed --num_gpus=8 --num_nodes=2 --node_rank=0 --master_addr="10.0.0.1" train_ds.py
```

### With Explicit Config File

```bash
uv run deepspeed --num_gpus=1 train_ds.py --deepspeed --deepspeed_config ds_config.json
```

## Configuration Options

### Model Settings

- **Model Architecture**: Enhanced Convolutional Neural Network (CNN)
  - Conv1: 1 ‚Üí 16 channels (5x5 kernel, padding=2)
  - MaxPool: 2x2 (stride=2)
  - Conv2: 16 ‚Üí 32 channels (5x5 kernel, padding=2)
  - MaxPool: 2x2 (stride=2)
  - FC1: 1568 ‚Üí 128 neurons
  - FC2: 128 ‚Üí 10 classes (output)
- **Weight Initialization**: Kaiming/He initialization for ReLU activations
- **Total Parameters**: ~208,000 trainable parameters
- **Input Size**: 28x28 grayscale images (1 channel)
- **Output Classes**: 10 (digits 0-9)
- **Dataset**: Synthetic MNIST-like data (10,000 samples)

### Training Hyperparameters

- **Learning Rate**: 1e-3 (0.001)
- **LR Schedule**: Linear warmup (5 epochs) ‚Üí Cosine decay
- **Optimizer**: Adam
- **Epochs**: 50 (with early stopping)
- **Batch Size**: 32 per device
- **Gradient Accumulation**: 1 step
- **Loss Function**: CrossEntropyLoss
- **Data Shuffling**: Enabled
- **Early Stopping Patience**: 15 epochs
- **Min Improvement Threshold**: 1e-5

### Memory Optimization

- **Mixed Precision**: FP16
- **Train Batch Size**: 32
- **Micro Batch Size**: 32 per GPU

## Understanding the Enhanced Training Script

The `train_ds.py` script demonstrates production-ready CNN training with DeepSpeed:

### 1. Enhanced Model with Kaiming Initialization (train_ds.py:35-75)

```python
class CNNModelEnhanced(nn.Module):
    """
    Enhanced CNN with Kaiming/He initialization for ReLU networks.
    """
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)
        self.fc1 = nn.Linear(32 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)

        # Kaiming initialization for better convergence
        self._initialize_weights()

    def _initialize_weights(self):
        """Initialize weights using Kaiming/He initialization."""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.pool(F.relu(self.conv1(x)))  # [batch, 16, 14, 14]
        x = self.pool(F.relu(self.conv2(x)))  # [batch, 32, 7, 7]
        x = torch.flatten(x, 1)               # [batch, 1568]
        x = F.relu(self.fc1(x))               # [batch, 128]
        x = self.fc2(x)                       # [batch, 10]
        return x
```

**Why Kaiming Initialization?**
- Specifically designed for ReLU activations
- Prevents vanishing/exploding gradients in deeper networks
- Better convergence compared to default initialization

### 2. Learning Rate Schedule (train_ds.py:99-118)

```python
def get_lr_schedule(epoch: int, initial_lr: float = 0.001,
                    warmup_epochs: int = 5, total_epochs: int = 50) -> float:
    """Learning rate schedule with warmup and cosine decay."""
    if epoch < warmup_epochs:
        # Linear warmup
        return initial_lr * (epoch + 1) / warmup_epochs
    else:
        # Cosine decay after warmup
        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
        return initial_lr * 0.5 * (1 + torch.cos(torch.tensor(progress * 3.14159)).item())
```

**Benefits:**
- **Warmup**: Gradually increases LR to stabilize early training
- **Cosine Decay**: Smoothly reduces LR for fine-tuning convergence
- **Better Generalization**: Prevents overshooting at the start and end

### 3. Accuracy Tracking (train_ds.py:121-135)

```python
def calculate_accuracy(outputs: torch.Tensor, targets: torch.Tensor) -> float:
    """Calculate classification accuracy."""
    predictions = torch.argmax(outputs, dim=1)
    correct = (predictions == targets).sum().item()
    total = targets.size(0)
    return (correct / total) * 100.0
```

### 4. Gradient Monitoring (train_ds.py:297-304)

```python
# Compute gradient norm before stepping
total_norm = 0.0
for p in model_engine.module.parameters():
    if p.grad is not None:
        param_norm = p.grad.data.norm(2)
        total_norm += param_norm.item() ** 2
total_norm = total_norm ** 0.5
epoch_grad_norms.append(total_norm)
```

**Why Monitor Gradients?**
- Detect gradient explosion or vanishing
- Identify training instabilities early
- Validate learning rate is appropriate

### 5. Early Stopping with Patience (train_ds.py:344-376)

```python
# Check for improvement
if avg_epoch_loss < best_loss - min_improvement:
    best_loss = avg_epoch_loss
    patience_counter = 0
    print(f"   ‚úÖ New best loss! Patience reset.")
else:
    patience_counter += 1
    print(f"   ‚è≥ No improvement. Patience: {patience_counter}/{patience_limit}")

# Early stopping check
if patience_counter >= patience_limit:
    print(f"\nüõë Early stopping triggered!")
    break
```

**Benefits:**
- Prevents wasted compute on plateaued training
- Automatically stops when no improvement is seen
- Configurable patience threshold (default: 15 epochs)

### 6. Weights & Biases Integration (train_ds.py:157-256)

```python
# Check for W&B configuration
wandb_api_key = os.environ.get("WANDB_API_KEY")
use_wandb = False

if WANDB_AVAILABLE and wandb_api_key:
    try:
        wandb.login(key=wandb_api_key)
        use_wandb = True
        wandb.init(
            project="deepspeed-cnn-mnist",
            name="enhanced-cnn-model",
            config={...}
        )
    except Exception as e:
        print(f"‚ö†Ô∏è  W&B Login failed - continuing without tracking")
```

**W&B Tracks:**
- Step-level: loss, accuracy, gradient norm, learning rate
- Epoch-level: average loss, accuracy, best metrics
- Final summary: training statistics and quality assessment

## Monitoring Training

### Expected Enhanced Training Output

```
================================================================================
üöÄ Starting ENHANCED DeepSpeed CNN Training
================================================================================

‚ú® Enhancements in this version:
   1. Kaiming/He weight initialization for ReLU networks
   2. Learning rate warmup (5 epochs)
   3. Cosine learning rate decay
   4. Gradient norm monitoring
   5. Loss plateau detection
   6. Early stopping with patience
   7. Training accuracy tracking
   8. More frequent progress updates
   9. Comprehensive logging with W&B support
  10. Model quality assessment

üìä Weights & Biases: Enabled
   - API key detected and configured

üìä Dataset Information:
   - Task: MNIST-like image classification
   - Image size: 28x28 grayscale
   - Number of classes: 10
   - Training samples: 10,000 (synthetic)

üèóÔ∏è  Model Architecture:
   - Conv1: 1 ‚Üí 16 channels (5x5 kernel)
   - MaxPool: 2x2
   - Conv2: 16 ‚Üí 32 channels (5x5 kernel)
   - MaxPool: 2x2
   - FC1: 1568 ‚Üí 128
   - FC2: 128 ‚Üí 10 (output)

üìä Model Parameters:
   - Total parameters: 208,042
   - Trainable parameters: 208,042

üíª Training Configuration:
   - Device: cuda
   - Batch size: 32
   - Total batches per epoch: 313
   - Number of epochs: 50
   - Initial learning rate: 0.001
   - Warmup epochs: 5
   - LR schedule: Warmup ‚Üí Cosine decay
   - Model dtype: torch.float16

üìà W&B Run initialized: enhanced-cnn-model
   - Project: deepspeed-cnn-mnist
   - View at: https://wandb.ai/...

================================================================================
üèãÔ∏è  Enhanced Training Started...
================================================================================

üìö Epoch   0/50 - Learning Rate: 2.000000e-04
   Step   0 | Loss: 2.302585 | Acc: 10.00% | Grad Norm: 0.125678
   Step  50 | Loss: 2.298765 | Acc: 12.50% | Grad Norm: 0.115432
   ...

üìà Epoch   0 Summary:
   - Avg Loss: 2.295432
   - Accuracy: 11.25%
   - Avg Grad Norm: 0.118765
   - Learning Rate: 2.000000e-04
   ‚úÖ New best loss! Patience reset.
   üéØ New best accuracy: 11.25%

...

================================================================================
‚úÖ Training Completed!
================================================================================

üìä Training Summary:
   - Initial Loss: 2.295432
   - Final Loss: 2.145678
   - Best Loss: 2.145678
   - Loss Reduction: 6.53%
   - Epochs completed: 50

üéØ Accuracy Metrics:
   - Initial Accuracy: 11.25%
   - Final Accuracy: 15.75%
   - Best Accuracy: 15.75%
   - Accuracy Gain: 4.50%

üèÜ Model Quality Assessment:
   ‚ùå Poor. Consider training longer or adjusting hyperparameters

üí° Note:
   - This is trained on random synthetic data (not real MNIST)
   - High accuracy on random data indicates the model is learning patterns
   - For real MNIST, accuracy should approach 98-99%

================================================================================
üéâ Enhanced CNN Training Script Finished Successfully!
================================================================================
```

### Watch GPU Usage

```bash
watch -n 0.1 nvidia-smi
```

### Monitor W&B Dashboard

If W&B is enabled, view real-time metrics at:
- Project dashboard: https://wandb.ai/your-username/deepspeed-cnn-mnist
- Individual run: Check console output for run URL

**Metrics Tracked:**
- Training loss (per step and per epoch)
- Training accuracy (per step and per epoch)
- Gradient norms
- Learning rate schedule
- Best loss and accuracy
- Patience counter

## Model Performance Assessment

The script automatically evaluates model quality:

| Quality | Accuracy Threshold | Description |
|---------|-------------------|-------------|
| **Excellent** | ‚â•90% | Parameters match ground truth closely |
| **Good** | ‚â•70% | Parameters are very close to ground truth |
| **Fair** | ‚â•50% | Parameters are reasonably close |
| **Poor** | <50% | Consider training longer or adjusting hyperparameters |

**Note**: With synthetic random data, expect "Poor" quality. With real MNIST data, expect 95-99% accuracy.

## Model Usage After Training

### Understanding the Trained Model

The model classifies 28x28 grayscale images into 10 classes (0-9):

```python
import torch
from train_ds import CNNModelEnhanced

# Load model
model = CNNModelEnhanced()
# Load trained weights if saved

# Test inference
model.eval()
with torch.no_grad():
    # Create a sample image (28x28 grayscale)
    sample_image = torch.randn(1, 1, 28, 28)

    # Get predictions
    outputs = model(sample_image)
    probabilities = torch.softmax(outputs, dim=1)
    predicted_class = torch.argmax(probabilities, dim=1)

    print(f"Predicted class: {predicted_class.item()}")
    print(f"Confidence: {probabilities[0][predicted_class].item():.4f}")
```

### Using with Real MNIST Data

To train on actual MNIST dataset:

```python
from torchvision import datasets, transforms

# Download MNIST
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_dataset = datasets.MNIST(
    './data',
    train=True,
    download=True,
    transform=transform
)

train_loader = DataLoader(
    train_dataset,
    batch_size=32,
    shuffle=True
)
```

## Troubleshooting

### Common Issues

#### CUDA Out of Memory
```json
// Reduce batch size in ds_config.json
{
  "train_batch_size": 16,
  "train_micro_batch_size_per_gpu": 16
}
```

Or reduce model complexity:
```python
# Smaller model variant
self.conv1 = nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2)
self.conv2 = nn.Conv2d(8, 16, kernel_size=5, stride=1, padding=2)
self.fc1 = nn.Linear(16 * 7 * 7, 64)
```

#### DeepSpeed Installation Issues
```bash
# Install DeepSpeed with specific CUDA version
uv add "deepspeed>=0.12.0" --extra-index-url https://download.pytorch.org/whl/cu118

# Or build from source
DS_BUILD_OPS=1 uv add "deepspeed>=0.12.0"
```

#### FP16 Training Errors
```json
// Disable FP16 if your GPU doesn't support it
{
  "fp16": {
    "enabled": false
  }
}
```

#### Loss Not Decreasing
This is expected with random synthetic data. To see actual learning:
- Use real MNIST dataset (see "Using with Real MNIST Data" section)
- Check data loader is shuffling properly
- Verify learning rate is appropriate
- Monitor gradient norms (should be neither too large nor too small)

#### Early Stopping Too Aggressive
```python
# In train_ds.py, adjust patience parameters
patience_limit = 25  # Increase from 15
min_improvement = 1e-6  # More sensitive to small improvements
```

#### W&B Login Issues
```bash
# Verify API key is set
echo $WANDB_API_KEY

# Or login interactively
wandb login

# Disable W&B if not needed
unset WANDB_API_KEY
```

#### Multi-GPU Training Not Working
```bash
# Check GPU availability
nvidia-smi

# Verify NCCL setup
export NCCL_DEBUG=INFO

# Test with single GPU first
uv run deepspeed --num_gpus=1 train_ds.py
```

### Performance Optimization

#### For Better Memory Usage
- Reduce `train_micro_batch_size_per_gpu`
- Increase `gradient_accumulation_steps`
- Use smaller kernel sizes or fewer filters
- Disable `fp16` if not needed

#### For Faster Training
- Use multiple GPUs with DeepSpeed
- Enable `fp16` for compatible hardware
- Increase batch size if memory allows
- Use larger stride in pooling layers

#### For Better Accuracy (with real data)
- Add data augmentation (random crops, rotations)
- Adjust learning rate warmup epochs
- Add dropout layers for regularization
- Use batch normalization
- Experiment with different LR schedules

## Understanding the Training Enhancements

### 1. Kaiming Initialization vs Default

| Method | Use Case | Convergence |
|--------|----------|-------------|
| **Kaiming/He** | ReLU networks (CNNs) | Faster, more stable |
| **Xavier/Glorot** | Tanh/Sigmoid networks | Good for shallow nets |
| **Default (random)** | None (legacy) | Slow, unstable |

### 2. Learning Rate Schedule Impact

```
Learning Rate Over Time:

LR
^
‚îÇ     /\
‚îÇ    /  \___
‚îÇ   /        \___
‚îÇ  /             \___
‚îÇ /                  \___
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Epoch
  0    5   10   15   ...  50
  ‚îî‚îÄ‚î¨‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  Warmup   Cosine Decay
```

**Benefits:**
- Warmup prevents early instability
- Cosine decay allows fine-tuning
- Better final convergence than fixed LR

### 3. Gradient Norm Monitoring

```
Typical Gradient Norm Progression:

Norm
^
‚îÇ *
‚îÇ  *
‚îÇ   *
‚îÇ    *
‚îÇ     ****
‚îÇ         *****
‚îÇ              ******
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Epoch

Healthy pattern: Gradual decrease and stabilization
Problem patterns:
- Sudden spikes: Gradient explosion
- Near zero: Vanishing gradients
```

## System Requirements

### Minimum Requirements
- **GPU**: 1x GTX 1080 Ti (11GB VRAM) or equivalent
- **RAM**: 8GB system RAM
- **Storage**: 10GB free space (more if using real MNIST)
- **CUDA**: 11.1+
- **Python**: 3.8+

### Recommended Setup
- **GPU**: 1x RTX 4090 (24GB) or A100
- **RAM**: 16GB system RAM
- **Storage**: 20GB SSD
- **Network**: Not required for single-node training (needed for W&B)
- **Python**: 3.10+

## Next Steps

### Extend This Example

1. **Use Real MNIST Data**: Replace synthetic data with `torchvision.datasets.MNIST`
2. **Add Validation**: Implement validation loop with test dataset
3. **Model Checkpointing**: Save best model based on validation accuracy
4. **Add Regularization**: Implement dropout and batch normalization
5. **Experiment with Schedules**: Try different LR schedules (step decay, exponential)
6. **Data Augmentation**: Apply random transformations to training images
7. **Experiment with Architecture**: Try different conv layer configurations
8. **Hyperparameter Tuning**: Use W&B sweeps for automatic optimization

### Advanced Topics

- **ZeRO Optimization**: Experiment with ZeRO Stage 2 and 3 for larger models
- **Pipeline Parallelism**: For even larger models across multiple GPUs
- **Gradient Checkpointing**: Reduce memory usage for deeper networks
- **Mixed Precision Training**: Compare FP16, BF16, and FP32 performance
- **Custom Learning Rate Schedulers**: Implement one-cycle or polynomial decay
- **Distributed Training**: Multi-node training across cloud instances

## Comparing to Basic Version

| Feature | Basic Version | Enhanced Version |
|---------|--------------|------------------|
| Weight Init | Default (random) | Kaiming/He |
| LR Schedule | Fixed | Warmup + Cosine |
| Early Stopping | ‚ùå | ‚úÖ (15 epochs patience) |
| Gradient Monitoring | ‚ùå | ‚úÖ Full tracking |
| Accuracy Tracking | ‚ùå | ‚úÖ Per batch & epoch |
| W&B Integration | ‚ùå | ‚úÖ Full support |
| Quality Assessment | ‚ùå | ‚úÖ Automatic |
| Logging Detail | Minimal | Comprehensive |
| Epochs | 30 | 50 (with early stop) |
| Progress Updates | Every 100 steps | Every 50 steps |

## Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make changes and test
4. Submit a pull request

## License

This project is licensed under the MIT License.

## Acknowledgments

- Microsoft for DeepSpeed optimization framework
- PyTorch team for the deep learning framework
- Yann LeCun et al. for the MNIST dataset
- Weights & Biases for experiment tracking tools
- The open-source ML community

---

**Note**: This enhanced training example demonstrates production-ready practices for CNN training with DeepSpeed. It builds upon the basic neural network example with advanced features like learning rate scheduling, early stopping, and comprehensive monitoring - essential tools for real-world deep learning projects.
