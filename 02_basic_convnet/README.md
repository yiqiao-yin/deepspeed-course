# Basic Convolutional Neural Network with DeepSpeed

Train a simple CNN for image classification using DeepSpeed on synthetic MNIST-like data.

## Features

- ðŸ–¼ï¸ **Image Classification**: CNN architecture for 28x28 grayscale image classification
- ðŸŽ¯ **MNIST-Style Task**: 10-class classification on synthetic data
- âš¡ **DeepSpeed Integration**: Distributed training with DeepSpeed optimization
- ðŸ”§ **FP16 Training**: Mixed precision training for faster computation
- ðŸ’» **Multi-GPU Ready**: Supports distributed training across multiple GPUs
- ðŸ“Š **Modern Architecture**: Conv layers + Pooling + Fully Connected layers

## Quick Start on RunPod

### 1. Initial Setup

Start with a fresh RunPod instance (recommend >= 1x RTX 4090 or A100):

```bash
# Install uv package manager
pip install uv

# Initialize new project
uv init basic-convnet-ds
cd basic-convnet-ds

# Add core dependencies
uv add "torch>=2.0.0"
uv add "deepspeed>=0.12.0"

# Development dependencies
uv add --dev "black" "isort" "flake8"
```

### 2. Project Structure

Create the following directory structure:

```
basic-convnet-ds/
â”œâ”€â”€ train_ds.py            # Your training script
â”œâ”€â”€ ds_config.json         # DeepSpeed configuration
â”œâ”€â”€ requirements.txt       # Generated by uv
â””â”€â”€ README.md             # This file
```

### 3. DeepSpeed Configuration

Create `ds_config.json`:

```json
{
  "train_batch_size": 32,
  "train_micro_batch_size_per_gpu": 32,
  "gradient_accumulation_steps": 1,
  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": 1e-3
    }
  },
  "fp16": {
    "enabled": true
  }
}
```

### 4. Add Your Training Script

Copy your `train_ds.py` script to the project directory.

## Running the Training

### Single GPU Training

```bash
uv run deepspeed --num_gpus=1 train_ds.py
```

### Multi-GPU Training with DeepSpeed

```bash
# For 2 GPUs
uv run deepspeed --num_gpus=2 train_ds.py

# For 4 GPUs
uv run deepspeed --num_gpus=4 train_ds.py

# For multi-node training
uv run deepspeed --num_gpus=8 --num_nodes=2 --node_rank=0 --master_addr="10.0.0.1" train_ds.py
```

### With Explicit Config File

```bash
uv run deepspeed --num_gpus=1 train_ds.py --deepspeed --deepspeed_config ds_config.json
```

## Configuration Options

### Model Settings

- **Model Architecture**: Convolutional Neural Network (CNN)
  - Conv1: 1 â†’ 16 channels (5x5 kernel, padding=2)
  - MaxPool: 2x2 (stride=2)
  - Conv2: 16 â†’ 32 channels (5x5 kernel, padding=2)
  - MaxPool: 2x2 (stride=2)
  - FC1: 1568 â†’ 128 neurons
  - FC2: 128 â†’ 10 classes (output)
- **Input Size**: 28x28 grayscale images (1 channel)
- **Output Classes**: 10 (digits 0-9)
- **Dataset**: Synthetic MNIST-like data (10,000 samples)

### Training Hyperparameters

- **Learning Rate**: 1e-3
- **Optimizer**: Adam
- **Epochs**: 30
- **Batch Size**: 32 per device
- **Gradient Accumulation**: 1 step
- **Loss Function**: CrossEntropyLoss
- **Data Shuffling**: Enabled

### Memory Optimization

- **Mixed Precision**: FP16
- **Train Batch Size**: 32
- **Micro Batch Size**: 32 per GPU

## Understanding the Training Script

The `train_ds.py` script demonstrates CNN training with DeepSpeed:

### 1. Model Architecture (train_ds.py:10-32)
```python
class CNNModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)
        self.fc1 = nn.Linear(32 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.pool(F.relu(self.conv1(x)))  # [batch, 16, 14, 14]
        x = self.pool(F.relu(self.conv2(x)))  # [batch, 32, 7, 7]
        x = torch.flatten(x, 1)               # [batch, 32*7*7]
        x = F.relu(self.fc1(x))               # [batch, 128]
        x = self.fc2(x)                       # [batch, 10]
        return x
```

**Architecture Flow:**
- Input: [batch, 1, 28, 28]
- After Conv1 + ReLU + Pool: [batch, 16, 14, 14]
- After Conv2 + ReLU + Pool: [batch, 32, 7, 7]
- After Flatten: [batch, 1568]
- After FC1 + ReLU: [batch, 128]
- Output: [batch, 10]

### 2. Data Generation (train_ds.py:35-45)
```python
def get_data_loader(batch_size: int) -> DataLoader:
    num_samples = 10000
    x_data = torch.randn(num_samples, 1, 28, 28)
    y_data = torch.randint(0, 10, (num_samples,))
    dataset = TensorDataset(x_data, y_data)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)
```

### 3. DeepSpeed Initialization (train_ds.py:54-58)
```python
model_engine, _, _, _ = deepspeed.initialize(
    model=model,
    model_parameters=model.parameters(),
    config="ds_config.json"
)
```

### 4. Training Loop (train_ds.py:66-78)
The training loop uses CrossEntropyLoss for multi-class classification and DeepSpeed's optimized backward pass.

## Monitoring Training

### Watch GPU Usage

```bash
watch -n 0.1 nvidia-smi
```

### Expected Training Output

```
Epoch 0 | Step 0 | Loss: 2.302585
Epoch 0 | Step 100 | Loss: 2.301234
Epoch 1 | Step 0 | Loss: 2.298765
Epoch 1 | Step 100 | Loss: 2.295432
...
```

Note: Since the data is random, loss may not decrease significantly. With real MNIST data, you'd see loss converging to near 0.

## Model Usage After Training

### Understanding the Trained Model

The model classifies 28x28 grayscale images into 10 classes (0-9):

```python
import torch
from train_ds import CNNModel

# Load model
model = CNNModel()
# Load trained weights if saved

# Test inference
model.eval()
with torch.no_grad():
    # Create a sample image (28x28 grayscale)
    sample_image = torch.randn(1, 1, 28, 28)

    # Get predictions
    outputs = model(sample_image)
    probabilities = torch.softmax(outputs, dim=1)
    predicted_class = torch.argmax(probabilities, dim=1)

    print(f"Predicted class: {predicted_class.item()}")
    print(f"Confidence: {probabilities[0][predicted_class].item():.4f}")
```

### Using with Real MNIST Data

To train on actual MNIST dataset:

```python
from torchvision import datasets, transforms

# Download MNIST
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_dataset = datasets.MNIST(
    './data',
    train=True,
    download=True,
    transform=transform
)

train_loader = DataLoader(
    train_dataset,
    batch_size=32,
    shuffle=True
)
```

## Troubleshooting

### Common Issues

#### CUDA Out of Memory
```json
// Reduce batch size in ds_config.json
{
  "train_batch_size": 16,
  "train_micro_batch_size_per_gpu": 16
}
```

Or reduce model complexity:
```python
# Smaller model variant
self.conv1 = nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2)
self.conv2 = nn.Conv2d(8, 16, kernel_size=5, stride=1, padding=2)
self.fc1 = nn.Linear(16 * 7 * 7, 64)
```

#### DeepSpeed Installation Issues
```bash
# Install DeepSpeed with specific CUDA version
uv add "deepspeed>=0.12.0" --extra-index-url https://download.pytorch.org/whl/cu118

# Or build from source
DS_BUILD_OPS=1 uv add "deepspeed>=0.12.0"
```

#### FP16 Training Errors
```json
// Disable FP16 if your GPU doesn't support it
{
  "fp16": {
    "enabled": false
  }
}
```

#### Loss Not Decreasing
This is expected with random synthetic data. To see actual learning:
- Use real MNIST dataset (see "Using with Real MNIST Data" section)
- Check data loader is shuffling properly
- Verify learning rate is appropriate

#### Multi-GPU Training Not Working
```bash
# Check GPU availability
nvidia-smi

# Verify NCCL setup
export NCCL_DEBUG=INFO

# Test with single GPU first
uv run deepspeed --num_gpus=1 train_ds.py
```

### Performance Optimization

#### For Better Memory Usage
- Reduce `train_micro_batch_size_per_gpu`
- Increase `gradient_accumulation_steps`
- Use smaller kernel sizes or fewer filters
- Disable `fp16` if not needed

#### For Faster Training
- Use multiple GPUs with DeepSpeed
- Enable `fp16` for compatible hardware
- Increase batch size if memory allows
- Use larger stride in pooling layers

#### For Better Accuracy (with real data)
- Add data augmentation (random crops, rotations)
- Implement learning rate scheduling
- Add dropout layers for regularization
- Use batch normalization

## System Requirements

### Minimum Requirements
- **GPU**: 1x GTX 1080 Ti (11GB VRAM) or equivalent
- **RAM**: 8GB system RAM
- **Storage**: 10GB free space (more if using real MNIST)
- **CUDA**: 11.1+

### Recommended Setup
- **GPU**: 1x RTX 4090 (24GB) or A100
- **RAM**: 16GB system RAM
- **Storage**: 20GB SSD
- **Network**: Not required for single-node training

## Next Steps

### Extend This Example

1. **Use Real MNIST Data**: Replace synthetic data with `torchvision.datasets.MNIST`
2. **Add Validation**: Implement validation loop with test dataset
3. **Model Checkpointing**: Save best model based on validation accuracy
4. **Add Regularization**: Implement dropout and batch normalization
5. **Learning Rate Scheduling**: Add cosine annealing or step decay
6. **Data Augmentation**: Apply random transformations to training images
7. **Experiment with Architecture**: Try different conv layer configurations

### Advanced Topics

- **ZeRO Optimization**: Experiment with ZeRO Stage 2 and 3
- **Pipeline Parallelism**: For even larger models
- **Gradient Checkpointing**: Reduce memory usage for deeper networks
- **Mixed Precision Training**: Compare FP16, BF16, and FP32

## Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make changes and test
4. Submit a pull request

## License

This project is licensed under the MIT License.

## Acknowledgments

- Microsoft for DeepSpeed optimization framework
- PyTorch team for the deep learning framework
- Yann LeCun et al. for the MNIST dataset
- The open-source ML community

---

**Note**: This training example is designed for educational purposes to demonstrate CNN training with DeepSpeed. It's an excellent next step after basic neural networks, introducing convolutional layers, pooling, and image classification concepts.
