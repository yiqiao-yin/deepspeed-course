"""Enhanced CIFAR-10 training script with improved convergence strategies.

Improvements over basic cifar10_deepspeed.py:
1. Better weight initialization (Kaiming/He for ReLU networks)
2. Learning rate warmup and decay
3. Gradient clipping with monitoring
4. Loss plateau detection and LR adjustment
5. Early stopping with patience
6. Gradient norm tracking
7. More frequent parameter monitoring
8. Training accuracy tracking
9. Comprehensive logging and W&B integration
10. Model quality assessment based on accuracy

This script trains a CNN on CIFAR-10 dataset (32x32 RGB images, 10 classes).
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import deepspeed
import sys
import os

# Optional Weights & Biases integration
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    wandb = None


class CIFAR10CNNEnhanced(nn.Module):
    """
    Enhanced CNN for CIFAR-10 classification with better initialization.
    Architecture adapted for 32x32 RGB images (3 channels).
    """

    def __init__(self):
        super().__init__()
        # CIFAR-10 has 3 color channels (RGB) instead of 1 (grayscale)
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)
        # After 2 pooling layers: 32x32 -> 16x16 -> 8x8
        self.fc1 = nn.Linear(64 * 8 * 8, 512)
        self.fc2 = nn.Linear(512, 10)

        # Kaiming/He initialization for ReLU networks (better for deep networks)
        self._initialize_weights()

    def _initialize_weights(self):
        """Initialize weights using Kaiming/He initialization for ReLU activations."""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for CNN.
        Input: [batch, 3, 32, 32]
        Output: [batch, 10] (logits)
        """
        x = self.pool(F.relu(self.conv1(x)))  # [batch, 32, 16, 16]
        x = self.pool(F.relu(self.conv2(x)))  # [batch, 64, 8, 8]
        x = F.relu(self.conv3(x))             # [batch, 64, 8, 8]
        x = torch.flatten(x, 1)               # [batch, 64*8*8 = 4096]
        x = F.relu(self.fc1(x))               # [batch, 512]
        x = self.fc2(x)                       # [batch, 10]
        return x


def get_cifar10_dataloaders(batch_size: int = 32):
    """
    Load CIFAR-10 dataset with standard transforms.

    Args:
        batch_size: Number of samples per batch

    Returns:
        Tuple of (train_loader, test_loader)
    """
    # CIFAR-10 standard normalization values
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    trainset = torchvision.datasets.CIFAR10(
        root='./data',
        train=True,
        download=True,
        transform=transform_train
    )

    testset = torchvision.datasets.CIFAR10(
        root='./data',
        train=False,
        download=True,
        transform=transform_test
    )

    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)
    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)

    return train_loader, test_loader


def get_lr_schedule(epoch: int, initial_lr: float = 0.001, warmup_epochs: int = 5, total_epochs: int = 50) -> float:
    """
    Learning rate schedule with warmup and cosine decay.

    Args:
        epoch: Current epoch
        initial_lr: Initial learning rate
        warmup_epochs: Number of warmup epochs
        total_epochs: Total training epochs

    Returns:
        Adjusted learning rate
    """
    if epoch < warmup_epochs:
        # Linear warmup
        return initial_lr * (epoch + 1) / warmup_epochs
    else:
        # Cosine decay after warmup
        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
        return initial_lr * 0.5 * (1 + torch.cos(torch.tensor(progress * 3.14159)).item())


def calculate_accuracy(outputs: torch.Tensor, targets: torch.Tensor) -> float:
    """
    Calculate classification accuracy.

    Args:
        outputs: Model output logits [batch, num_classes]
        targets: Ground truth labels [batch]

    Returns:
        Accuracy as percentage (0-100)
    """
    predictions = torch.argmax(outputs, dim=1)
    correct = (predictions == targets).sum().item()
    total = targets.size(0)
    return (correct / total) * 100.0


def main() -> None:
    """
    Enhanced CIFAR-10 training with multiple convergence strategies.
    """

    print("=" * 80)
    print("üöÄ Starting ENHANCED DeepSpeed CIFAR-10 Training")
    print("=" * 80)
    print("\n‚ú® Enhancements in this version:")
    print("   1. Kaiming/He weight initialization for ReLU networks")
    print("   2. Learning rate warmup (5 epochs)")
    print("   3. Cosine learning rate decay")
    print("   4. Gradient norm monitoring")
    print("   5. Loss plateau detection")
    print("   6. Early stopping with patience")
    print("   7. Training accuracy tracking")
    print("   8. More frequent progress updates")
    print("   9. Comprehensive logging with W&B support")
    print("  10. Model quality assessment")

    # Check for Weights & Biases configuration
    wandb_api_key = os.environ.get("WANDB_API_KEY")
    use_wandb = False

    if WANDB_AVAILABLE and wandb_api_key:
        try:
            wandb.login(key=wandb_api_key)
            use_wandb = True
            print(f"\n‚úÖ Weights & Biases: Enabled")
            print(f"   - API key detected and configured")
        except Exception as e:
            print(f"\n‚ö†Ô∏è  Weights & Biases: Login failed - {e}")
            print(f"   - Continuing without W&B tracking")
            use_wandb = False
    elif WANDB_AVAILABLE and not wandb_api_key:
        print(f"\nüìä Weights & Biases: Not configured")
        print(f"   - To enable: export WANDB_API_KEY=your_api_key")
    elif not WANDB_AVAILABLE:
        print(f"\nüìä Weights & Biases: Not installed")
        print(f"   - To enable tracking: pip install wandb")

    # CIFAR-10 classes
    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

    print(f"\nüìä Dataset Information:")
    print(f"   - Dataset: CIFAR-10")
    print(f"   - Image size: 32x32 RGB")
    print(f"   - Number of classes: 10")
    print(f"   - Classes: {', '.join(classes)}")
    print(f"   - Training samples: 50,000")
    print(f"   - Test samples: 10,000")

    model = CIFAR10CNNEnhanced()

    print(f"\nüèóÔ∏è  Model Architecture:")
    print(f"   - Conv1: 3 ‚Üí 32 channels (3x3 kernel)")
    print(f"   - MaxPool: 2x2")
    print(f"   - Conv2: 32 ‚Üí 64 channels (3x3 kernel)")
    print(f"   - MaxPool: 2x2")
    print(f"   - Conv3: 64 ‚Üí 64 channels (3x3 kernel)")
    print(f"   - FC1: 4096 ‚Üí 512")
    print(f"   - FC2: 512 ‚Üí 10 (output)")

    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nüìä Model Parameters:")
    print(f"   - Total parameters: {total_params:,}")
    print(f"   - Trainable parameters: {trainable_params:,}")

    print(f"\n‚öôÔ∏è  Initializing DeepSpeed...")
    model_engine, optimizer, _, _ = deepspeed.initialize(
        model=model,
        model_parameters=model.parameters(),
        config="ds_config.json"
    )
    print(f"‚úÖ DeepSpeed initialized successfully")

    batch_size = model_engine.train_micro_batch_size_per_gpu()
    train_loader, test_loader = get_cifar10_dataloaders(batch_size=batch_size)
    device = model_engine.device

    print(f"\nüíª Training Configuration:")
    print(f"   - Device: {device}")
    print(f"   - Batch size: {batch_size}")
    print(f"   - Total batches per epoch: {len(train_loader)}")
    print(f"   - Number of epochs: 50")
    print(f"   - Initial learning rate: 0.001")
    print(f"   - Warmup epochs: 5")
    print(f"   - LR schedule: Warmup ‚Üí Cosine decay")

    model_dtype = next(model_engine.module.parameters()).dtype
    print(f"   - Model dtype: {model_dtype}")

    # Initialize W&B run if enabled
    if use_wandb:
        wandb.init(
            project="deepspeed-cifar10",
            name="enhanced-cifar10-cnn",
            config={
                "model": "EnhancedCIFAR10CNN",
                "dataset": "CIFAR-10",
                "num_classes": 10,
                "epochs": 50,
                "batch_size": batch_size,
                "optimizer": "Adam",
                "initial_lr": 0.001,
                "warmup_epochs": 5,
                "lr_schedule": "warmup_cosine",
                "initialization": "kaiming",
                "framework": "DeepSpeed",
                "total_params": total_params,
                "trainable_params": trainable_params,
                "data_augmentation": ["random_crop", "random_horizontal_flip"],
                "enhancements": [
                    "kaiming_init",
                    "lr_warmup",
                    "cosine_decay",
                    "gradient_monitoring",
                    "early_stopping",
                    "accuracy_tracking"
                ]
            }
        )
        print(f"\nüìà W&B Run initialized: {wandb.run.name}")
        print(f"   - Project: deepspeed-cifar10")
        print(f"   - View at: {wandb.run.url}")

    print(f"\n{'='*80}")
    print("üèãÔ∏è  Enhanced Training Started...")
    print(f"{'='*80}\n")

    loss_fn = nn.CrossEntropyLoss()
    epoch_losses = []
    epoch_accuracies = []
    best_loss = float('inf')
    best_accuracy = 0.0
    patience_counter = 0
    patience_limit = 15
    min_improvement = 1e-5
    total_epochs = 50

    for epoch in range(total_epochs):
        epoch_loss_sum = 0.0
        epoch_correct = 0
        epoch_total = 0
        num_batches = 0
        epoch_grad_norms = []

        # Get learning rate for this epoch
        current_lr = get_lr_schedule(epoch, initial_lr=0.001, warmup_epochs=5, total_epochs=total_epochs)

        # Update optimizer learning rate
        for param_group in optimizer.param_groups:
            param_group['lr'] = current_lr

        print(f"\nüìö Epoch {epoch:3d}/{total_epochs} - Learning Rate: {current_lr:.6e}")

        model_engine.train()
        for step, (x_batch, y_batch) in enumerate(train_loader):
            x_batch = x_batch.to(device).to(model_dtype)
            y_batch = y_batch.to(device)

            outputs = model_engine(x_batch)
            loss = loss_fn(outputs, y_batch)

            model_engine.backward(loss)

            # Compute gradient norm before stepping
            total_norm = 0.0
            for p in model_engine.module.parameters():
                if p.grad is not None:
                    param_norm = p.grad.data.norm(2)
                    total_norm += param_norm.item() ** 2
            total_norm = total_norm ** 0.5
            epoch_grad_norms.append(total_norm)

            model_engine.step()

            # Track metrics
            epoch_loss_sum += loss.item()
            batch_accuracy = calculate_accuracy(outputs, y_batch)
            batch_correct = int((batch_accuracy / 100.0) * y_batch.size(0))
            epoch_correct += batch_correct
            epoch_total += y_batch.size(0)
            num_batches += 1

            if step % 100 == 0:  # Update every 100 steps
                print(f"   Step {step:4d} | Loss: {loss.item():.6f} | Acc: {batch_accuracy:.2f}% | Grad Norm: {total_norm:.6f}")

                # Log to W&B if enabled
                if use_wandb:
                    wandb.log({
                        "step_loss": loss.item(),
                        "step_accuracy": batch_accuracy,
                        "gradient_norm": total_norm,
                        "learning_rate": current_lr,
                        "epoch": epoch,
                        "step": step
                    })

        # Calculate average metrics for the epoch
        avg_epoch_loss = epoch_loss_sum / num_batches
        avg_grad_norm = sum(epoch_grad_norms) / len(epoch_grad_norms)
        epoch_accuracy = (epoch_correct / epoch_total) * 100.0
        epoch_losses.append(avg_epoch_loss)
        epoch_accuracies.append(epoch_accuracy)

        # Print epoch summary
        print(f"\nüìà Epoch {epoch:3d} Summary:")
        print(f"   - Avg Loss: {avg_epoch_loss:.6f}")
        print(f"   - Accuracy: {epoch_accuracy:.2f}%")
        print(f"   - Avg Grad Norm: {avg_grad_norm:.6f}")
        print(f"   - Learning Rate: {current_lr:.6e}")

        # Check for improvement
        if avg_epoch_loss < best_loss - min_improvement:
            best_loss = avg_epoch_loss
            patience_counter = 0
            print(f"   ‚úÖ New best loss! Patience reset.")
        else:
            patience_counter += 1
            print(f"   ‚è≥ No improvement. Patience: {patience_counter}/{patience_limit}")

        # Track best accuracy
        if epoch_accuracy > best_accuracy:
            best_accuracy = epoch_accuracy
            print(f"   üéØ New best accuracy: {best_accuracy:.2f}%")

        # Log epoch metrics to W&B
        if use_wandb:
            wandb.log({
                "epoch": epoch,
                "epoch_avg_loss": avg_epoch_loss,
                "epoch_accuracy": epoch_accuracy,
                "epoch_avg_grad_norm": avg_grad_norm,
                "learning_rate": current_lr,
                "best_loss": best_loss,
                "best_accuracy": best_accuracy,
                "patience": patience_counter
            })

        # Early stopping check
        if patience_counter >= patience_limit:
            print(f"\nüõë Early stopping triggered! No improvement for {patience_limit} epochs.")
            print(f"   Best loss achieved: {best_loss:.6f}")
            print(f"   Best accuracy achieved: {best_accuracy:.2f}%")
            break

    print(f"\n{'='*80}")
    print("‚úÖ Training Completed!")
    print(f"{'='*80}\n")

    # Final results
    final_loss = epoch_losses[-1]
    final_accuracy = epoch_accuracies[-1]
    initial_loss = epoch_losses[0]
    initial_accuracy = epoch_accuracies[0]
    loss_reduction = ((initial_loss - final_loss) / initial_loss) * 100
    accuracy_gain = final_accuracy - initial_accuracy

    print(f"üìä Training Summary:")
    print(f"   - Initial Loss: {initial_loss:.6f}")
    print(f"   - Final Loss: {final_loss:.6f}")
    print(f"   - Best Loss: {best_loss:.6f}")
    print(f"   - Loss Reduction: {loss_reduction:.2f}%")
    print(f"   - Epochs completed: {epoch + 1}")

    print(f"\nüéØ Accuracy Metrics:")
    print(f"   - Initial Accuracy: {initial_accuracy:.2f}%")
    print(f"   - Final Accuracy: {final_accuracy:.2f}%")
    print(f"   - Best Accuracy: {best_accuracy:.2f}%")
    print(f"   - Accuracy Gain: {accuracy_gain:.2f}%")

    # Model quality assessment
    quality_score = "excellent" if best_accuracy >= 80 else \
                   "good" if best_accuracy >= 70 else \
                   "fair" if best_accuracy >= 60 else "poor"

    print(f"\nüèÜ Model Quality Assessment:")
    if quality_score == "excellent":
        print(f"   ‚ú® Excellent! Model achieved ‚â•80% accuracy on CIFAR-10")
    elif quality_score == "good":
        print(f"   ‚úÖ Good! Model achieved ‚â•70% accuracy on CIFAR-10")
    elif quality_score == "fair":
        print(f"   ‚ö†Ô∏è  Fair. Model achieved ‚â•60% accuracy on CIFAR-10")
    else:
        print(f"   ‚ùå Poor. Consider training longer or adjusting hyperparameters")

    # CIFAR-10 specific notes
    print(f"\nüí° Note:")
    print(f"   - CIFAR-10 is a real-world dataset with natural images")
    print(f"   - Good models typically achieve 75-85% accuracy")
    print(f"   - State-of-the-art models can reach 95%+ with deeper architectures")
    print(f"   - Current model is relatively simple (for demonstration)")

    # Log final summary to W&B
    if use_wandb:
        wandb.log({
            "final/loss": final_loss,
            "final/best_loss": best_loss,
            "final/loss_reduction_pct": loss_reduction,
            "final/accuracy": final_accuracy,
            "final/best_accuracy": best_accuracy,
            "final/accuracy_gain": accuracy_gain,
            "final/quality_score": quality_score,
            "final/epochs_completed": epoch + 1
        })

        # Create a summary table
        wandb.run.summary["best_loss"] = best_loss
        wandb.run.summary["best_accuracy"] = best_accuracy
        wandb.run.summary["final_accuracy"] = final_accuracy
        wandb.run.summary["total_loss_reduction"] = loss_reduction
        wandb.run.summary["accuracy_gain"] = accuracy_gain
        wandb.run.summary["quality"] = quality_score

        print(f"\nüìä W&B Summary logged")
        print(f"   - View results at: {wandb.run.url}")

        # Finish W&B run
        wandb.finish()
        print(f"   - W&B run finished successfully")

    print(f"\n{'='*80}")
    print("üéâ Enhanced CIFAR-10 Training Script Finished Successfully!")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()
