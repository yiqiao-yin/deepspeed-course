# CIFAR-10 CNN Training with DeepSpeed

Train a convolutional neural network on the CIFAR-10 dataset using DeepSpeed for distributed training with advanced optimization features.

## Features

- 🖼️ **Real Dataset**: Trains on CIFAR-10 (60,000 32x32 color images in 10 classes)
- 🚀 **Advanced Optimization**: WarmupLR scheduler with Adam optimizer
- ⚡ **DeepSpeed Integration**: Multi-GPU distributed training support
- 📊 **Production-Ready**: Automatic data loading, normalization, and preprocessing
- 🎯 **10-Class Classification**: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck
- 🔧 **Flexible Training**: Two ways to launch training (direct command or Python launcher)

## Quick Start on RunPod

### 1. Initial Setup

Start with a fresh RunPod instance (recommend >= 2x RTX 4090 or A100):

```bash
# Install uv package manager
pip install uv

# Initialize new project
uv init cifar10-deepspeed
cd cifar10-deepspeed

# Add core dependencies
uv add "torch>=2.0.0"
uv add "torchvision>=0.15.0"
uv add "deepspeed>=0.12.0"

# Development dependencies
uv add --dev "black" "isort" "flake8"
```

### 2. Project Structure

Create the following directory structure:

```
cifar10-deepspeed/
├── cifar10_deepspeed.py      # Main training script
├── cifar10_main.py            # Python launcher (optional)
├── ds_config.json             # DeepSpeed configuration
├── requirements.txt           # Generated by uv
├── README.md                  # This file
└── data/                      # CIFAR-10 dataset (auto-downloaded)
```

### 3. DeepSpeed Configuration

Create `ds_config.json`:

```json
{
  "train_batch_size": 4,
  "steps_per_print": 2000,
  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": 0.001,
      "betas": [0.8, 0.999],
      "eps": 1e-8,
      "weight_decay": 3e-7
    }
  },
  "scheduler": {
    "type": "WarmupLR",
    "params": {
      "warmup_min_lr": 0,
      "warmup_max_lr": 0.001,
      "warmup_num_steps": 1000
    }
  },
  "wall_clock_breakdown": false
}
```

### 4. Add Your Training Scripts

Copy the training scripts to your project directory:
- `cifar10_deepspeed.py` - Main training script
- `cifar10_main.py` - Optional Python launcher for convenience

## Running the Training

### Method 1: Direct DeepSpeed Command (Recommended)

```bash
# Single GPU
uv run deepspeed --num_gpus=1 cifar10_deepspeed.py --deepspeed --deepspeed_config ds_config.json

# Multi-GPU (2 GPUs)
uv run deepspeed --num_gpus=2 cifar10_deepspeed.py --deepspeed --deepspeed_config ds_config.json

# Multi-GPU (4 GPUs)
uv run deepspeed --num_gpus=4 cifar10_deepspeed.py --deepspeed --deepspeed_config ds_config.json
```

### Method 2: Using Python Launcher

```bash
# Edit cifar10_main.py to set desired number of GPUs (default: 2)
uv run python cifar10_main.py
```

The launcher script (cifar10_main.py:11-17) automatically executes:
```python
cmd = [
    "deepspeed",
    "--num_gpus=2",
    "cifar10_deepspeed.py",
    "--deepspeed",
    "--deepspeed_config", "ds_config.json"
]
```

### Custom Training Options

```bash
# Train for more epochs
uv run deepspeed --num_gpus=2 cifar10_deepspeed.py --deepspeed --deepspeed_config ds_config.json --epochs 10

# Multi-node training
uv run deepspeed --num_gpus=8 --num_nodes=2 --node_rank=0 --master_addr="10.0.0.1" cifar10_deepspeed.py --deepspeed --deepspeed_config ds_config.json
```

## Configuration Options

### Model Architecture

The model uses a classic CNN architecture (cifar10_deepspeed.py:31-47):

```python
class Net(nn.Module):
    def __init__(self):
        self.conv1 = nn.Conv2d(3, 6, 5)      # 3 input channels (RGB) → 6 output
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)     # 6 → 16 channels
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)         # 10 output classes
```

**Architecture Flow:**
- Input: [batch, 3, 32, 32] (RGB images)
- After Conv1 + ReLU + Pool: [batch, 6, 14, 14]
- After Conv2 + ReLU + Pool: [batch, 16, 5, 5]
- After Flatten: [batch, 400]
- After FC1 + ReLU: [batch, 120]
- After FC2 + ReLU: [batch, 84]
- Output: [batch, 10] (class logits)

### Dataset Settings

- **Dataset**: CIFAR-10
- **Training samples**: 50,000 images
- **Test samples**: 10,000 images
- **Image size**: 32x32 RGB
- **Classes**: 10 (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)
- **Normalization**: Mean=(0.5, 0.5, 0.5), Std=(0.5, 0.5, 0.5)

### Training Hyperparameters

- **Optimizer**: Adam
  - Learning Rate: 0.001
  - Betas: [0.8, 0.999]
  - Epsilon: 1e-8
  - Weight Decay: 3e-7
- **Learning Rate Scheduler**: WarmupLR
  - Warmup steps: 1000
  - Min LR: 0
  - Max LR: 0.001
- **Batch Size**: 4 (total across all GPUs)
- **Default Epochs**: 2 (configurable via `--epochs`)
- **Loss Function**: CrossEntropyLoss
- **Print Interval**: Every 2000 steps

### DeepSpeed Optimization Features

- **Automatic Data Distribution**: DeepSpeed handles data parallelism
- **Warmup Learning Rate**: Gradual LR increase for stable training
- **Optimized Adam**: DeepSpeed's fused Adam optimizer
- **Distributed Training**: Automatic multi-GPU coordination

## Understanding the Training Scripts

### Main Training Script (cifar10_deepspeed.py)

#### 1. Argument Parsing (cifar10_deepspeed.py:12-18)
```python
def add_argument():
    parser = argparse.ArgumentParser(description='CIFAR10 with DeepSpeed')
    parser.add_argument('--deepspeed_config', type=str, default='ds_config.json')
    parser.add_argument('--epochs', type=int, default=2)
    parser.add_argument('--local_rank', type=int, default=-1)
    parser = deepspeed.add_config_arguments(parser)
    return parser.parse_args()
```

#### 2. Data Preparation (cifar10_deepspeed.py:20-28)
Automatically downloads CIFAR-10 dataset to `./data` directory with proper normalization.

#### 3. DeepSpeed Initialization (cifar10_deepspeed.py:57-62)
```python
model_engine, optimizer, trainloader, _ = deepspeed.initialize(
    args=args,
    model=model,
    model_parameters=parameters,
    training_data=trainset
)
```

DeepSpeed automatically:
- Creates the optimizer from config
- Sets up the learning rate scheduler
- Creates distributed data loader
- Initializes multi-GPU communication

#### 4. Training Loop (cifar10_deepspeed.py:64-76)
Uses DeepSpeed's optimized backward pass and automatic gradient accumulation.

### Python Launcher (cifar10_main.py)

A convenience script that programmatically launches DeepSpeed training:
- Simplifies command execution
- Easy to modify for different configurations
- Useful for automation and scripting

## Monitoring Training

### Watch GPU Usage

```bash
watch -n 0.1 nvidia-smi
```

### Expected Training Output

```
Files already downloaded and verified
Files already downloaded and verified
[1,  2000] loss: 2.156
[1,  4000] loss: 1.987
[1,  6000] loss: 1.823
[1,  8000] loss: 1.701
[1, 10000] loss: 1.612
[1, 12000] loss: 1.541
[2,  2000] loss: 1.468
[2,  4000] loss: 1.421
...
Finished Training
```

### Understanding the Output

- First two lines: CIFAR-10 dataset download/verification status
- Loss decreases as training progresses (good convergence)
- With 50,000 training images and batch_size=4:
  - Each epoch has ~12,500 steps
  - Progress printed every 2000 steps

## Model Evaluation

Add evaluation code to test model performance:

```python
# Add this after training in cifar10_deepspeed.py

def evaluate(model_engine, testset):
    """Evaluate model on test set"""
    testloader = torch.utils.data.DataLoader(
        testset, batch_size=4, shuffle=False, num_workers=2
    )

    correct = 0
    total = 0

    model_engine.module.eval()
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(model_engine.device), data[1].to(model_engine.device)
            outputs = model_engine(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    print(f'Accuracy on 10,000 test images: {accuracy:.2f}%')
    return accuracy

# Call after training
evaluate(model_engine, testset)
```

## Saving and Loading Models

### Save Model Checkpoint

```python
# Add after training
model_engine.save_checkpoint('./checkpoints', tag='final')
```

### Load and Use Trained Model

```python
import torch
from cifar10_deepspeed import Net

# Load model
model = Net()
checkpoint = torch.load('./checkpoints/final/mp_rank_00_model_states.pt')
model.load_state_dict(checkpoint['module'])
model.eval()

# Make predictions
classes = ('plane', 'car', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck')

with torch.no_grad():
    outputs = model(test_images)
    _, predicted = torch.max(outputs, 1)
    print(f'Predicted: {classes[predicted[0]]}')
```

## Troubleshooting

### Common Issues

#### CUDA Out of Memory
```json
// Reduce batch size in ds_config.json
{
  "train_batch_size": 2
}
```

Or use gradient accumulation:
```json
{
  "train_batch_size": 4,
  "gradient_accumulation_steps": 2
}
```

#### Dataset Download Fails
```bash
# Manual download
mkdir -p ./data
# Download from https://www.cs.toronto.edu/~kriz/cifar.html
# Or use VPN if blocked in your region
```

#### DeepSpeed Installation Issues
```bash
# Install DeepSpeed with specific CUDA version
uv add "deepspeed>=0.12.0" --extra-index-url https://download.pytorch.org/whl/cu118

# Or build from source
DS_BUILD_OPS=1 uv add "deepspeed>=0.12.0"
```

#### Low Accuracy After Training
- Increase number of epochs: `--epochs 20`
- Adjust learning rate in `ds_config.json`
- Add data augmentation (random flips, crops)
- Increase model capacity (more filters, layers)

#### Multi-GPU Training Not Working
```bash
# Check GPU availability
nvidia-smi

# Verify NCCL setup
export NCCL_DEBUG=INFO

# Test with single GPU first
uv run deepspeed --num_gpus=1 cifar10_deepspeed.py --deepspeed --deepspeed_config ds_config.json
```

#### Warmup Scheduler Issues
If training is unstable:
```json
// Adjust warmup steps in ds_config.json
{
  "scheduler": {
    "type": "WarmupLR",
    "params": {
      "warmup_num_steps": 2000  // Increase for more gradual warmup
    }
  }
}
```

### Performance Optimization

#### For Better Memory Usage
- Reduce `train_batch_size` in config
- Add `gradient_accumulation_steps`
- Use gradient checkpointing for deeper models
- Reduce model capacity (fewer filters)

#### For Faster Training
- Use multiple GPUs with DeepSpeed
- Increase batch size if memory allows
- Enable mixed precision (add FP16 config)
- Increase `num_workers` in DataLoader

#### For Better Accuracy
- Train for more epochs (20-50 recommended)
- Add data augmentation:
  ```python
  transform = transforms.Compose([
      transforms.RandomHorizontalFlip(),
      transforms.RandomCrop(32, padding=4),
      transforms.ToTensor(),
      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
  ])
  ```
- Use learning rate decay
- Implement early stopping
- Try different optimizers (SGD with momentum)

## Advanced Configuration

### Adding ZeRO Optimization

Add to `ds_config.json`:

```json
{
  "zero_optimization": {
    "stage": 2,
    "allgather_partitions": true,
    "allgather_bucket_size": 2e8,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 2e8,
    "contiguous_gradients": true
  }
}
```

### Adding FP16 Training

Add to `ds_config.json`:

```json
{
  "fp16": {
    "enabled": true,
    "loss_scale": 0,
    "initial_scale_power": 16,
    "loss_scale_window": 1000,
    "hysteresis": 2,
    "min_loss_scale": 1
  }
}
```

### Adding Gradient Clipping

Add to `ds_config.json`:

```json
{
  "gradient_clipping": 1.0
}
```

## System Requirements

### Minimum Requirements
- **GPU**: 1x GTX 1080 Ti (11GB VRAM) or equivalent
- **RAM**: 16GB system RAM
- **Storage**: 500MB for dataset + 10GB for working space
- **CUDA**: 11.1+
- **Internet**: Required for initial dataset download

### Recommended Setup
- **GPU**: 2x RTX 4090 (24GB) or 2x A100 (40GB)
- **RAM**: 32GB system RAM
- **Storage**: 50GB SSD
- **Network**: High-bandwidth for multi-node training

## Expected Results

With default configuration (2 epochs):
- **Training Time**: ~5-10 minutes on 2x RTX 4090
- **Final Loss**: ~1.3-1.5
- **Test Accuracy**: ~60-65%

With extended training (20 epochs + data augmentation):
- **Training Time**: ~50-100 minutes
- **Final Loss**: ~0.5-0.8
- **Test Accuracy**: ~75-80%

## Next Steps

### Improve Model Performance

1. **Data Augmentation**: Add random flips, crops, rotations
2. **Extended Training**: Increase epochs to 20-50
3. **Better Architecture**: Try ResNet, VGG, or other modern architectures
4. **Regularization**: Add dropout, batch normalization
5. **Hyperparameter Tuning**: Grid search for optimal LR, weight decay

### Production Deployment

1. **Model Export**: Convert to ONNX or TorchScript
2. **Quantization**: Reduce model size with INT8 quantization
3. **Serving**: Deploy with TorchServe or TensorRT
4. **Monitoring**: Add MLflow or Weights & Biases tracking

## Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make changes and test
4. Submit a pull request

## License

This project is licensed under the MIT License.

## Acknowledgments

- Alex Krizhevsky for the CIFAR-10 dataset
- Microsoft for DeepSpeed optimization framework
- PyTorch and torchvision teams
- The open-source ML community

## References

- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)
- [DeepSpeed Documentation](https://www.deepspeed.ai/)
- [PyTorch Tutorials](https://pytorch.org/tutorials/)
- [Learning Multiple Layers of Features from Tiny Images](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf) - Original CIFAR paper

---

**Note**: This is a production-ready example demonstrating real-world image classification with DeepSpeed. It showcases advanced features like learning rate warmup, automatic data loading, and distributed training coordination. Perfect for learning distributed training with real datasets.
