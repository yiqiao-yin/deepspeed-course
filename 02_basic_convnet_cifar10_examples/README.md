# Enhanced CIFAR-10 CNN Training with DeepSpeed

Train an enhanced CNN on the CIFAR-10 dataset using DeepSpeed with production-ready training features and comprehensive monitoring.

## Features

- üñºÔ∏è **Real Dataset**: CIFAR-10 (50,000 train + 10,000 test, 32x32 RGB images)
- üéØ **10-Class Classification**: plane, car, bird, cat, deer, dog, frog, horse, ship, truck
- ‚ö° **DeepSpeed Integration**: Multi-GPU distributed training with optimization
- üîß **FP16 Training**: Mixed precision training for faster computation
- üíª **Multi-GPU Ready**: Supports distributed training across multiple GPUs
- üìä **Enhanced Architecture**: 3 conv layers + 2 FC layers with proper initialization
- üéì **Kaiming Initialization**: Better weight initialization for ReLU networks
- üìà **Learning Rate Scheduling**: Warmup + Cosine decay for optimal convergence
- üõë **Early Stopping**: Patience-based early stopping to prevent overfitting
- üìâ **Gradient Monitoring**: Track gradient norms throughout training
- üéØ **Accuracy Tracking**: Real-time training accuracy calculation and logging
- üîç **Loss Plateau Detection**: Automatic detection of training plateaus
- üìä **W&B Integration**: Optional Weights & Biases tracking and visualization
- üèÜ **Quality Assessment**: Automatic model performance evaluation
- üñºÔ∏è **Data Augmentation**: Random crop + horizontal flip for better generalization

## Quick Start on RunPod

### 1. Initial Setup

Start with a fresh RunPod instance (recommend >= 1x RTX 4090 or A100):

```bash
# Install uv package manager
pip install uv

# Initialize new project
uv init cifar10-deepspeed
cd cifar10-deepspeed

# Add core dependencies
uv add "torch>=2.0.0"
uv add "torchvision>=0.15.0"
uv add "deepspeed>=0.12.0"

# Optional: Add Weights & Biases for tracking
uv add "wandb"

# Development dependencies
uv add --dev "black" "isort" "flake8"
```

### 2. Project Structure

Create the following directory structure:

```
cifar10-deepspeed/
‚îú‚îÄ‚îÄ cifar10_deepspeed.py      # Enhanced training script
‚îú‚îÄ‚îÄ ds_config.json             # DeepSpeed configuration
‚îú‚îÄ‚îÄ requirements.txt           # Generated by uv
‚îú‚îÄ‚îÄ README.md                  # This file
‚îî‚îÄ‚îÄ data/                      # CIFAR-10 dataset (auto-downloaded)
```

### 3. DeepSpeed Configuration

Create `ds_config.json`:

```json
{
  "train_batch_size": 32,
  "train_micro_batch_size_per_gpu": 32,
  "gradient_accumulation_steps": 1,
  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": 1e-3
    }
  },
  "fp16": {
    "enabled": true
  }
}
```

**Note**: We removed the built-in `scheduler` from the config because the training script implements manual learning rate warmup + cosine decay. This gives us more control over the learning rate schedule.

### 4. Add Your Training Script

Copy `cifar10_deepspeed.py` to your project directory.

### 5. (Optional) Configure Weights & Biases

To enable experiment tracking with W&B:

```bash
# Set your W&B API key
export WANDB_API_KEY=your_api_key_here

# Or configure it interactively
wandb login
```

## Running the Training

### Single GPU Training

```bash
uv run deepspeed --num_gpus=1 cifar10_deepspeed.py
```

### Multi-GPU Training with DeepSpeed

```bash
# For 2 GPUs
uv run deepspeed --num_gpus=2 cifar10_deepspeed.py

# For 4 GPUs
uv run deepspeed --num_gpus=4 cifar10_deepspeed.py

# For multi-node training
uv run deepspeed --num_gpus=8 --num_nodes=2 --node_rank=0 --master_addr="10.0.0.1" cifar10_deepspeed.py
```

**Note**: The script automatically uses `ds_config.json` from the same directory. No need to pass config file arguments.

## Configuration Options

### Model Settings

- **Model Architecture**: Enhanced CNN for CIFAR-10
  - Conv1: 3 ‚Üí 32 channels (3x3 kernel, padding=1)
  - MaxPool: 2x2 (stride=2)
  - Conv2: 32 ‚Üí 64 channels (3x3 kernel, padding=1)
  - MaxPool: 2x2 (stride=2)
  - Conv3: 64 ‚Üí 64 channels (3x3 kernel, padding=1)
  - FC1: 4096 ‚Üí 512
  - FC2: 512 ‚Üí 10 (output)
- **Weight Initialization**: Kaiming/He initialization for ReLU activations
- **Total Parameters**: ~2,100,000 trainable parameters
- **Input Size**: 32x32 RGB images (3 channels)
- **Output Classes**: 10 (plane, car, bird, cat, deer, dog, frog, horse, ship, truck)
- **Dataset**: CIFAR-10 (50,000 train + 10,000 test images)

### Training Hyperparameters

- **Learning Rate**: 1e-3 (0.001)
- **LR Schedule**: Linear warmup (5 epochs) ‚Üí Cosine decay
- **Optimizer**: Adam
- **Epochs**: 50 (with early stopping)
- **Batch Size**: 32 per device
- **Gradient Accumulation**: 1 step
- **Loss Function**: CrossEntropyLoss
- **Data Shuffling**: Enabled for training
- **Early Stopping Patience**: 15 epochs
- **Min Improvement Threshold**: 1e-5

### Data Augmentation

**Training:**
- Random horizontal flip
- Random crop (32x32 with padding=4)
- Standard CIFAR-10 normalization

**Testing:**
- No augmentation
- Standard CIFAR-10 normalization only

### Memory Optimization

- **Mixed Precision**: FP16
- **Train Batch Size**: 32
- **Micro Batch Size**: 32 per GPU

## Understanding the Enhanced Training Script

The `cifar10_deepspeed.py` script demonstrates production-ready CIFAR-10 training with DeepSpeed:

### 1. Enhanced Model with Kaiming Initialization (cifar10_deepspeed.py:38-82)

```python
class CIFAR10CNNEnhanced(nn.Module):
    """
    Enhanced CNN for CIFAR-10 with Kaiming/He initialization.
    Architecture adapted for 32x32 RGB images (3 channels).
    """
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(64 * 8 * 8, 512)
        self.fc2 = nn.Linear(512, 10)

        # Kaiming initialization for better convergence
        self._initialize_weights()

    def _initialize_weights(self):
        """Initialize weights using Kaiming/He initialization."""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.pool(F.relu(self.conv1(x)))  # [batch, 32, 16, 16]
        x = self.pool(F.relu(self.conv2(x)))  # [batch, 64, 8, 8]
        x = F.relu(self.conv3(x))             # [batch, 64, 8, 8]
        x = torch.flatten(x, 1)               # [batch, 4096]
        x = F.relu(self.fc1(x))               # [batch, 512]
        x = self.fc2(x)                       # [batch, 10]
        return x
```

**Why Kaiming Initialization?**
- Specifically designed for ReLU activations
- Prevents vanishing/exploding gradients in deeper networks
- Better convergence compared to default initialization

### 2. CIFAR-10 Data Loading with Augmentation (cifar10_deepspeed.py:85-125)

```python
def get_cifar10_dataloaders(batch_size: int = 32):
    """Load CIFAR-10 dataset with standard transforms."""
    # Training transforms with augmentation
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    # Test transforms without augmentation
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                            download=True, transform=transform_train)
    testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                           download=True, transform=transform_test)

    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)
    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)

    return train_loader, test_loader
```

**Data Augmentation Benefits:**
- Random crop increases spatial robustness
- Horizontal flip doubles training diversity
- Standard normalization matches CIFAR-10 statistics

### 3. Learning Rate Schedule (cifar10_deepspeed.py:128-148)

```python
def get_lr_schedule(epoch: int, initial_lr: float = 0.001,
                    warmup_epochs: int = 5, total_epochs: int = 50) -> float:
    """Learning rate schedule with warmup and cosine decay."""
    if epoch < warmup_epochs:
        # Linear warmup
        return initial_lr * (epoch + 1) / warmup_epochs
    else:
        # Cosine decay after warmup
        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
        return initial_lr * 0.5 * (1 + torch.cos(torch.tensor(progress * 3.14159)).item())
```

**Benefits:**
- **Warmup**: Gradually increases LR to stabilize early training
- **Cosine Decay**: Smoothly reduces LR for fine-tuning convergence
- **Better Generalization**: Prevents overshooting at the start and end

### 4. Accuracy Tracking (cifar10_deepspeed.py:151-165)

```python
def calculate_accuracy(outputs: torch.Tensor, targets: torch.Tensor) -> float:
    """Calculate classification accuracy."""
    predictions = torch.argmax(outputs, dim=1)
    correct = (predictions == targets).sum().item()
    total = targets.size(0)
    return (correct / total) * 100.0
```

### 5. Gradient Monitoring (cifar10_deepspeed.py:345-352)

```python
# Compute gradient norm before stepping
total_norm = 0.0
for p in model_engine.module.parameters():
    if p.grad is not None:
        param_norm = p.grad.data.norm(2)
        total_norm += param_norm.item() ** 2
total_norm = total_norm ** 0.5
epoch_grad_norms.append(total_norm)
```

**Why Monitor Gradients?**
- Detect gradient explosion or vanishing
- Identify training instabilities early
- Validate learning rate is appropriate

### 6. Early Stopping with Patience (cifar10_deepspeed.py:392-424)

```python
# Check for improvement
if avg_epoch_loss < best_loss - min_improvement:
    best_loss = avg_epoch_loss
    patience_counter = 0
    print(f"   ‚úÖ New best loss! Patience reset.")
else:
    patience_counter += 1
    print(f"   ‚è≥ No improvement. Patience: {patience_counter}/{patience_limit}")

# Early stopping check
if patience_counter >= patience_limit:
    print(f"\nüõë Early stopping triggered!")
    break
```

**Benefits:**
- Prevents wasted compute on plateaued training
- Automatically stops when no improvement is seen
- Configurable patience threshold (default: 15 epochs)

### 7. Weights & Biases Integration (cifar10_deepspeed.py:196-303)

```python
# Check for W&B configuration
wandb_api_key = os.environ.get("WANDB_API_KEY")
use_wandb = False

if WANDB_AVAILABLE and wandb_api_key:
    try:
        wandb.login(key=wandb_api_key)
        use_wandb = True
        wandb.init(
            project="deepspeed-cifar10",
            name="enhanced-cifar10-cnn",
            config={...}
        )
    except Exception as e:
        print(f"‚ö†Ô∏è  W&B Login failed - continuing without tracking")
```

**W&B Tracks:**
- Step-level: loss, accuracy, gradient norm, learning rate
- Epoch-level: average loss, accuracy, best metrics
- Final summary: training statistics and quality assessment

## Monitoring Training

### Expected Enhanced Training Output

```
================================================================================
üöÄ Starting ENHANCED DeepSpeed CIFAR-10 Training
================================================================================

‚ú® Enhancements in this version:
   1. Kaiming/He weight initialization for ReLU networks
   2. Learning rate warmup (5 epochs)
   3. Cosine learning rate decay
   4. Gradient norm monitoring
   5. Loss plateau detection
   6. Early stopping with patience
   7. Training accuracy tracking
   8. More frequent progress updates
   9. Comprehensive logging with W&B support
  10. Model quality assessment

üìä Weights & Biases: Enabled
   - API key detected and configured

üìä Dataset Information:
   - Dataset: CIFAR-10
   - Image size: 32x32 RGB
   - Number of classes: 10
   - Classes: plane, car, bird, cat, deer, dog, frog, horse, ship, truck
   - Training samples: 50,000
   - Test samples: 10,000

üèóÔ∏è  Model Architecture:
   - Conv1: 3 ‚Üí 32 channels (3x3 kernel)
   - MaxPool: 2x2
   - Conv2: 32 ‚Üí 64 channels (3x3 kernel)
   - MaxPool: 2x2
   - Conv3: 64 ‚Üí 64 channels (3x3 kernel)
   - FC1: 4096 ‚Üí 512
   - FC2: 512 ‚Üí 10 (output)

üìä Model Parameters:
   - Total parameters: 2,109,386
   - Trainable parameters: 2,109,386

üíª Training Configuration:
   - Device: cuda
   - Batch size: 32
   - Total batches per epoch: 1,563
   - Number of epochs: 50
   - Initial learning rate: 0.001
   - Warmup epochs: 5
   - LR schedule: Warmup ‚Üí Cosine decay
   - Model dtype: torch.float16

üìà W&B Run initialized: enhanced-cifar10-cnn
   - Project: deepspeed-cifar10
   - View at: https://wandb.ai/...

================================================================================
üèãÔ∏è  Enhanced Training Started...
================================================================================

üìö Epoch   0/50 - Learning Rate: 2.000000e-04
   Step    0 | Loss: 2.302585 | Acc: 10.00% | Grad Norm: 0.185432
   Step  100 | Loss: 2.195678 | Acc: 15.62% | Grad Norm: 0.165234
   ...

üìà Epoch   0 Summary:
   - Avg Loss: 2.145678
   - Accuracy: 18.45%
   - Avg Grad Norm: 0.158765
   - Learning Rate: 2.000000e-04
   ‚úÖ New best loss! Patience reset.
   üéØ New best accuracy: 18.45%

...

üìà Epoch  25 Summary:
   - Avg Loss: 0.856432
   - Accuracy: 72.34%
   - Avg Grad Norm: 0.045678
   - Learning Rate: 5.234567e-04
   ‚úÖ New best loss! Patience reset.
   üéØ New best accuracy: 72.34%

...

================================================================================
‚úÖ Training Completed!
================================================================================

üìä Training Summary:
   - Initial Loss: 2.145678
   - Final Loss: 0.789234
   - Best Loss: 0.765432
   - Loss Reduction: 63.24%
   - Epochs completed: 45

üéØ Accuracy Metrics:
   - Initial Accuracy: 18.45%
   - Final Accuracy: 74.56%
   - Best Accuracy: 75.23%
   - Accuracy Gain: 56.11%

üèÜ Model Quality Assessment:
   ‚úÖ Good! Model achieved ‚â•70% accuracy on CIFAR-10

üí° Note:
   - CIFAR-10 is a real-world dataset with natural images
   - Good models typically achieve 75-85% accuracy
   - State-of-the-art models can reach 95%+ with deeper architectures
   - Current model is relatively simple (for demonstration)

================================================================================
üéâ Enhanced CIFAR-10 Training Script Finished Successfully!
================================================================================
```

### Watch GPU Usage

```bash
watch -n 0.1 nvidia-smi
```

### Monitor W&B Dashboard

If W&B is enabled, view real-time metrics at:
- Project dashboard: https://wandb.ai/your-username/deepspeed-cifar10
- Individual run: Check console output for run URL

**Metrics Tracked:**
- Training loss (per step and per epoch)
- Training accuracy (per step and per epoch)
- Gradient norms
- Learning rate schedule
- Best loss and accuracy
- Patience counter

## Model Performance Assessment

The script automatically evaluates model quality:

| Quality | Accuracy Threshold | Description |
|---------|-------------------|-------------|
| **Excellent** | ‚â•80% | High-quality CNN for CIFAR-10 |
| **Good** | ‚â•70% | Solid performance on CIFAR-10 |
| **Fair** | ‚â•60% | Acceptable baseline performance |
| **Poor** | <60% | Consider training longer or adjusting hyperparameters |

**Expected Performance:**
- Simple CNNs: 70-80% accuracy
- ResNet-18/34: 85-90% accuracy
- State-of-the-art: 95%+ accuracy with deeper architectures

## Saving and Loading Models

### Save Model Checkpoint

```python
# Add after training in main()
model_engine.save_checkpoint('./checkpoints', tag='best_model')
```

### Load and Use Trained Model

```python
import torch
from cifar10_deepspeed import CIFAR10CNNEnhanced

# Load model
model = CIFAR10CNNEnhanced()
checkpoint = torch.load('./checkpoints/best_model/mp_rank_00_model_states.pt')
model.load_state_dict(checkpoint['module'])
model.eval()

# CIFAR-10 classes
classes = ('plane', 'car', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck')

# Make predictions
with torch.no_grad():
    outputs = model(test_images)
    _, predicted = torch.max(outputs, 1)
    print(f'Predicted: {classes[predicted[0]]}')
```

## Troubleshooting

### Common Issues

#### CUDA Out of Memory
```json
// Reduce batch size in ds_config.json
{
  "train_batch_size": 16,
  "train_micro_batch_size_per_gpu": 16
}
```

Or reduce model complexity:
```python
# Smaller model variant
self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
self.fc1 = nn.Linear(32 * 8 * 8, 256)
```

#### DeepSpeed Installation Issues
```bash
# Install DeepSpeed with specific CUDA version
uv add "deepspeed>=0.12.0" --extra-index-url https://download.pytorch.org/whl/cu118

# Or build from source
DS_BUILD_OPS=1 uv add "deepspeed>=0.12.0"
```

#### FP16 Training Errors
```json
// Disable FP16 if your GPU doesn't support it
{
  "fp16": {
    "enabled": false
  }
}
```

#### Dataset Download Fails
```bash
# Manual download
mkdir -p ./data
# Download from https://www.cs.toronto.edu/~kriz/cifar.html
# Or use VPN if blocked in your region
```

#### Early Stopping Too Aggressive
```python
# In cifar10_deepspeed.py, adjust patience parameters
patience_limit = 25  # Increase from 15
min_improvement = 1e-6  # More sensitive to small improvements
```

#### W&B Login Issues
```bash
# Verify API key is set
echo $WANDB_API_KEY

# Or login interactively
wandb login

# Disable W&B if not needed
unset WANDB_API_KEY
```

#### Multi-GPU Training Not Working
```bash
# Check GPU availability
nvidia-smi

# Verify NCCL setup
export NCCL_DEBUG=INFO

# Test with single GPU first
uv run deepspeed --num_gpus=1 cifar10_deepspeed.py
```

### Performance Optimization

#### For Better Memory Usage
- Reduce `train_micro_batch_size_per_gpu`
- Increase `gradient_accumulation_steps`
- Use smaller kernel sizes or fewer filters
- Disable `fp16` if not needed

#### For Faster Training
- Use multiple GPUs with DeepSpeed
- Enable `fp16` for compatible hardware
- Increase batch size if memory allows
- Increase `num_workers` in DataLoader

#### For Better Accuracy (CIFAR-10 Specific)
- Train for more epochs (50-100 recommended)
- Adjust learning rate warmup epochs
- Add dropout layers for regularization
- Use batch normalization
- Try data augmentation variations (rotation, color jitter)
- Experiment with different architectures (ResNet, VGG)

## Understanding the Training Enhancements

### 1. Kaiming Initialization vs Default

| Method | Use Case | Convergence |
|--------|----------|-------------|
| **Kaiming/He** | ReLU networks (CNNs) | Faster, more stable |
| **Xavier/Glorot** | Tanh/Sigmoid networks | Good for shallow nets |
| **Default (random)** | None (legacy) | Slow, unstable |

### 2. Learning Rate Schedule Impact

```
Learning Rate Over Time:

LR
^
‚îÇ     /\
‚îÇ    /  \___
‚îÇ   /        \___
‚îÇ  /             \___
‚îÇ /                  \___
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Epoch
  0    5   10   15   ...  50
  ‚îî‚îÄ‚î¨‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  Warmup   Cosine Decay
```

**Benefits:**
- Warmup prevents early instability
- Cosine decay allows fine-tuning
- Better final convergence than fixed LR

### 3. Gradient Norm Monitoring

```
Typical Gradient Norm Progression:

Norm
^
‚îÇ *
‚îÇ  *
‚îÇ   *
‚îÇ    *
‚îÇ     ****
‚îÇ         *****
‚îÇ              ******
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Epoch

Healthy pattern: Gradual decrease and stabilization
Problem patterns:
- Sudden spikes: Gradient explosion
- Near zero: Vanishing gradients
```

## System Requirements

### Minimum Requirements
- **GPU**: 1x GTX 1080 Ti (11GB VRAM) or equivalent
- **RAM**: 16GB system RAM
- **Storage**: 500MB for dataset + 10GB working space
- **CUDA**: 11.1+
- **Python**: 3.8+
- **Internet**: Required for initial dataset download

### Recommended Setup
- **GPU**: 1x RTX 4090 (24GB) or A100
- **RAM**: 32GB system RAM
- **Storage**: 50GB SSD
- **Network**: Not required for single-node training (needed for W&B)
- **Python**: 3.10+

## Expected Results

### With Enhanced Training (50 epochs):
- **Training Time**: ~30-60 minutes on RTX 4090
- **Final Loss**: ~0.7-0.9
- **Test Accuracy**: 75-80%
- **Quality**: Good to Excellent

### With Extended Training (100 epochs + tuning):
- **Training Time**: ~60-120 minutes
- **Final Loss**: ~0.5-0.7
- **Test Accuracy**: 80-85%
- **Quality**: Excellent

## Next Steps

### Extend This Example

1. **Add Validation Loop**: Implement proper train/val/test splits
2. **Model Checkpointing**: Save best model based on validation accuracy
3. **Add Regularization**: Implement dropout and batch normalization
4. **Experiment with Architectures**: Try ResNet, VGG, or DenseNet
5. **Hyperparameter Tuning**: Use W&B sweeps for automatic optimization
6. **Advanced Augmentation**: Add rotation, color jitter, cutout
7. **Learning Rate Schedules**: Try one-cycle or step decay

### Advanced Topics

- **ZeRO Optimization**: Experiment with ZeRO Stage 2 and 3 for larger models
- **Pipeline Parallelism**: For even larger models across multiple GPUs
- **Gradient Checkpointing**: Reduce memory usage for deeper networks
- **Mixed Precision Training**: Compare FP16, BF16, and FP32 performance
- **Custom Schedulers**: Implement one-cycle or polynomial decay
- **Distributed Training**: Multi-node training across cloud instances

## Comparing to Basic Version

| Feature | Basic Version | Enhanced Version |
|---------|--------------|------------------|
| Weight Init | Default (random) | Kaiming/He |
| LR Schedule | Fixed (0.001) | Warmup + Cosine |
| Early Stopping | ‚ùå | ‚úÖ (15 epochs patience) |
| Gradient Monitoring | ‚ùå | ‚úÖ Full tracking |
| Accuracy Tracking | ‚ùå | ‚úÖ Per batch & epoch |
| W&B Integration | ‚ùå | ‚úÖ Full support |
| Quality Assessment | ‚ùå | ‚úÖ Automatic |
| Data Augmentation | ‚ùå | ‚úÖ Crop + flip |
| Logging Detail | Minimal | Comprehensive |
| Epochs | 2 | 50 (with early stop) |
| Batch Size | 4 | 32 |
| Progress Updates | Every 2000 steps | Every 100 steps |
| Model Size | ~62K params | ~2.1M params |
| Expected Accuracy | 60-65% | 75-80% |

## Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make changes and test
4. Submit a pull request

## License

This project is licensed under the MIT License.

## Acknowledgments

- Alex Krizhevsky for the CIFAR-10 dataset
- Microsoft for DeepSpeed optimization framework
- PyTorch and torchvision teams
- Weights & Biases for experiment tracking tools
- The open-source ML community

## References

- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)
- [DeepSpeed Documentation](https://www.deepspeed.ai/)
- [PyTorch Tutorials](https://pytorch.org/tutorials/)
- [Learning Multiple Layers of Features from Tiny Images](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf) - Original CIFAR paper

---

**Note**: This enhanced training example demonstrates production-ready practices for CIFAR-10 CNN training with DeepSpeed. It includes advanced features like learning rate scheduling, early stopping, data augmentation, and comprehensive monitoring - essential tools for real-world deep learning projects on challenging datasets.
