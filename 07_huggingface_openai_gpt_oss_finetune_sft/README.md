# GPT-OSS-20B Multilingual Reasoning Fine-tuner

Fine-tune OpenAI GPT-OSS-20B on multilingual reasoning using LoRA, TRL SFTTrainer, and DeepSpeed for distributed training.

## Features

- ðŸš€ **Parameter-Efficient**: Uses LoRA (Low-Rank Adaptation) for memory-efficient fine-tuning
- ðŸŒ **Multilingual**: Trains on HuggingFaceH4/Multilingual-Thinking dataset
- âš¡ **Distributed Training**: DeepSpeed ZeRO for multi-GPU training
- ðŸ¤— **Hub Integration**: Automatic model upload to Hugging Face Hub
- ðŸ“Š **Monitoring**: TensorBoard logging and progress tracking

## Quick Start on RunPod

### 1. Initial Setup

Start with a fresh RunPod instance (recommend >= 2x RTX 4090 or A100):

```bash
# Install uv package manager
pip install uv

# Initialize new project
uv init gpt-oss-finetune
cd gpt-oss-finetune

# Add core dependencies
uv add "torch>=2.0.0"
uv add "transformers>=4.55.0"
uv add "trl>=0.20.0"
uv add "peft>=0.17.0"
uv add "datasets>=2.14.0"
uv add "deepspeed>=0.12.0"
uv add "accelerate>=0.24.0"
uv add "huggingface-hub>=0.17.0"
uv add "tensorboard"
uv add "trackio"

# Development dependencies
uv add --dev "black" "isort" "flake8"
```

### 2. Project Structure

Create the following directory structure:

```
gpt-oss-finetune/
â”œâ”€â”€ main.py                 # Your fine-tuning script
â”œâ”€â”€ ds_config.json         # DeepSpeed configuration
â”œâ”€â”€ requirements.txt       # Generated by uv
â”œâ”€â”€ README.md             # This file
â””â”€â”€ outputs/              # Training outputs directory
```

### 3. DeepSpeed Configuration

Create `ds_config.json`:

```json
{
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "zero_optimization": {
        "stage": 2,
        "allgather_partitions": true,
        "allgather_bucket_size": 2e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2e8,
        "contiguous_gradients": true,
        "cpu_offload": false
    },
    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 10,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```

### 4. Environment Configuration

Set up your environment variables:

```bash
# Required: Hugging Face token for model access and upload
export HF_TOKEN="your_huggingface_token_here"

# Optional: Configuration flags
export PUSH_TO_HUB="true"          # Enable/disable hub upload
export RUN_EVALUATION="true"       # Enable/disable post-training evaluation
export CUDA_VISIBLE_DEVICES="0,1"  # Specify GPUs to use
```

Get your HF token from: https://huggingface.co/settings/tokens

### 5. Add Your Training Script

Copy your `main.py` script to the project directory.

## Running the Training

### Single GPU Training

```bash
uv run python main.py
```

### Multi-GPU Training with DeepSpeed

```bash
# For 2 GPUs
uv run deepspeed --num_gpus=2 main.py

# For 4 GPUs
uv run deepspeed --num_gpus=4 main.py

# For multi-node training
uv run deepspeed --num_gpus=8 --num_nodes=2 --node_rank=0 --master_addr="10.0.0.1" main.py
```

### Using Accelerate (Alternative)

```bash
# Configure accelerate
uv run accelerate config

# Run training
uv run accelerate launch main.py
```

## Configuration Options

### Model Settings

- **Base Model**: `openai/gpt-oss-20b`
- **Dataset**: `HuggingFaceH4/Multilingual-Thinking`
- **LoRA Rank**: 8 (configurable in script)
- **LoRA Alpha**: 16

### Training Hyperparameters

- **Learning Rate**: 2e-4
- **Epochs**: 10
- **Batch Size**: 2 per device (with gradient accumulation)
- **Max Length**: 2048 tokens
- **Scheduler**: Cosine with minimum LR

### Memory Optimization

- **Gradient Checkpointing**: Enabled
- **Mixed Precision**: BF16
- **DeepSpeed ZeRO Stage 2**: For parameter sharding

## Output Structure

After training, you'll find:

```
outputs/
â”œâ”€â”€ gpt-oss-20b-multilingual-reasoner-ultra/
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ tokenizer.json
â””â”€â”€ runs/                    # TensorBoard logs
```

## Monitoring Training

View training progress with TensorBoard:

```bash
uv run tensorboard --logdir=outputs/runs
```

Access at: http://localhost:6006

## Model Usage After Training

### Loading the Fine-tuned Model

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load base model and tokenizer
base_model = AutoModelForCausalLM.from_pretrained(
    "openai/gpt-oss-20b",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("openai/gpt-oss-20b")

# Load and merge LoRA adapter
model = PeftModel.from_pretrained(base_model, "path/to/your/adapter")
model = model.merge_and_unload()

# Generate text
messages = [
    {"role": "system", "content": "reasoning language: German"},
    {"role": "user", "content": "Â¿CuÃ¡l es el capital de Australia?"}
]

input_ids = tokenizer.apply_chat_template(
    messages, 
    add_generation_prompt=True, 
    return_tensors="pt"
)

outputs = model.generate(
    input_ids,
    max_new_tokens=512,
    temperature=0.6,
    top_p=0.9,
    do_sample=True
)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## Troubleshooting

### Common Issues

#### CUDA Out of Memory
```bash
# Reduce batch size in main.py
per_device_train_batch_size=1
gradient_accumulation_steps=16

# Enable CPU offloading in ds_config.json
"cpu_offload": true
```

#### DeepSpeed Installation Issues
```bash
# Install DeepSpeed with specific CUDA version
uv add "deepspeed>=0.12.0" --extra-index-url https://download.pytorch.org/whl/cu118
```

#### Model Access Issues
```bash
# Ensure you have access to the model
huggingface-cli login

# Or set token directly
export HF_TOKEN="your_token"
```

#### Multi-GPU Training Not Working
```bash
# Check GPU availability
nvidia-smi

# Verify NCCL setup
export NCCL_DEBUG=INFO

# Test with smaller setup
uv run deepspeed --num_gpus=1 main.py
```

### Performance Optimization

#### For Better Memory Usage
- Increase `gradient_accumulation_steps`
- Reduce `per_device_train_batch_size`
- Enable `cpu_offload` in DeepSpeed config
- Use ZeRO Stage 3 for larger models

#### For Faster Training
- Use multiple GPUs with DeepSpeed
- Optimize `max_length` based on your data
- Adjust `dataloader_num_workers`

## System Requirements

### Minimum Requirements
- **GPU**: 1x RTX 4090 (24GB VRAM) or equivalent
- **RAM**: 32GB system RAM
- **Storage**: 100GB free space
- **CUDA**: 11.8+

### Recommended Setup
- **GPU**: 2x A100 (80GB) or 4x RTX 4090
- **RAM**: 128GB system RAM
- **Storage**: 500GB NVMe SSD
- **Network**: High-bandwidth for multi-node

## Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make changes and test
4. Submit a pull request

## License

This project is licensed under the MIT License.

## Acknowledgments

- OpenAI for the GPT-OSS-20B model
- Hugging Face for the transformers ecosystem
- Microsoft for DeepSpeed optimization
- The open-source ML community

---

**Note**: This fine-tuning setup is designed for research and educational purposes. Ensure you comply with the model's license terms and your organization's AI usage policies.