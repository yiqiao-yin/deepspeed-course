# Basic Neural Network with DeepSpeed

Train a simple linear regression model using DeepSpeed for distributed training on synthetic data.

## Features

- 🎯 **Simple & Educational**: Perfect introduction to DeepSpeed with minimal complexity
- 📊 **Linear Regression**: Trains y = 2x + 1 model on synthetic data
- ⚡ **DeepSpeed Integration**: Demonstrates core DeepSpeed initialization and training loop
- 🔧 **FP16 Training**: Mixed precision training for faster computation
- 💻 **Multi-GPU Ready**: Supports distributed training across multiple GPUs
- 📈 **Comprehensive Logging**: Detailed training progress with parameter convergence tracking
- 🎯 **Ground Truth Validation**: Automatic comparison of learned parameters vs. true values
- 🔄 **Optional W&B Tracking**: Seamless Weights & Biases integration (never crashes if not configured)

## Quick Start on RunPod

### 1. Initial Setup

Start with a fresh RunPod instance (recommend >= 1x RTX 4090 or A100):

```bash
# Install uv package manager
pip install uv

# Initialize new project
uv init basic-neuralnet-ds
cd basic-neuralnet-ds

# Add core dependencies
uv add "torch>=2.0.0"
uv add "deepspeed>=0.12.0"

# Optional: Weights & Biases for experiment tracking
uv add "wandb"

# Development dependencies
uv add --dev "black" "isort" "flake8"
```

### 2. Project Structure

Create the following directory structure:

```
basic-neuralnet-ds/
├── train_ds.py            # Your training script
├── ds_config.json         # DeepSpeed configuration
├── requirements.txt       # Generated by uv
└── README.md             # This file
```

### 3. DeepSpeed Configuration

Create `ds_config.json`:

```json
{
  "train_batch_size": 32,
  "train_micro_batch_size_per_gpu": 32,
  "gradient_accumulation_steps": 1,
  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": 1e-3
    }
  },
  "fp16": {
    "enabled": true
  }
}
```

### 4. Add Your Training Script

Copy your `train_ds.py` script to the project directory.

### 5. (Optional) Configure Weights & Biases

If you want to track experiments with W&B:

```bash
# Get your API key from https://wandb.ai/authorize
export WANDB_API_KEY="your_api_key_here"
```

**Note**: The script will work perfectly fine without W&B configured. It will simply show a helpful message and continue training.

## Running the Training

### Basic Training (without W&B)

```bash
# Single GPU
uv run deepspeed --num_gpus=1 train_ds.py

# Multi-GPU (2 GPUs)
uv run deepspeed --num_gpus=2 train_ds.py

# Multi-GPU (4 GPUs)
uv run deepspeed --num_gpus=4 train_ds.py
```

### Training with Weights & Biases Tracking

```bash
# Set your W&B API key
export WANDB_API_KEY="your_api_key_here"

# Run training (single GPU)
uv run deepspeed --num_gpus=1 train_ds.py

# Or multi-GPU
uv run deepspeed --num_gpus=4 train_ds.py
```

### Multi-Node Training

```bash
# For multi-node training
uv run deepspeed --num_gpus=8 --num_nodes=2 --node_rank=0 --master_addr="10.0.0.1" train_ds.py
```

### With Explicit Config File

```bash
uv run deepspeed --num_gpus=1 train_ds.py --deepspeed --deepspeed_config ds_config.json
```

## Configuration Options

### Model Settings

- **Model Architecture**: Simple Linear Layer (1 input → 1 output)
- **Task**: Linear regression y = 2x + 1
- **Dataset**: Synthetic data (1000 samples, randomly generated)

### Training Hyperparameters

- **Learning Rate**: 1e-3
- **Optimizer**: Adam
- **Epochs**: 30
- **Batch Size**: 32 per device
- **Gradient Accumulation**: 1 step
- **Loss Function**: MSE (Mean Squared Error)

### Memory Optimization

- **Mixed Precision**: FP16
- **Train Batch Size**: 32
- **Micro Batch Size**: 32 per GPU

## Understanding the Training Script

The `train_ds.py` script demonstrates core DeepSpeed concepts:

### 1. Model Definition (train_ds.py:9-22)
```python
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(1, 1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.linear(x)
```

### 2. DeepSpeed Initialization (train_ds.py:42-46)
```python
model_engine, _, _, _ = deepspeed.initialize(
    model=model,
    model_parameters=model.parameters(),
    config="ds_config.json"
)
```

### 3. Training Loop (train_ds.py:53-65)
The training loop uses DeepSpeed's `backward()` and `step()` methods instead of standard PyTorch optimizer calls.

## Monitoring Training

### Training Output

The script provides comprehensive logging with detailed progress information:

```
================================================================================
🚀 Starting DeepSpeed Linear Regression Training
================================================================================

✅ Weights & Biases: Enabled
   - API key detected and configured

📊 Dataset Information:
   - Synthetic data: y = 2.0x + 1.0
   - Training samples: 1000
   - True Weight (W): 2.0
   - True Bias (b): 1.0

🎲 Initial Model Parameters (random):
   - Weight: 0.456789
   - Bias: -0.123456

⚙️  Initializing DeepSpeed...
✅ DeepSpeed initialized successfully

💻 Training Configuration:
   - Device: cuda
   - Batch size: 32
   - Total batches per epoch: 32
   - Number of epochs: 30
   - Model dtype: torch.float16

📈 W&B Run initialized: simple-linear-model
   - Project: deepspeed-linear-regression
   - View at: https://wandb.ai/your-username/deepspeed-linear-regression/runs/abc123

================================================================================
🏋️  Training Started...
================================================================================

Epoch  0/30 | Step   0 | Loss: 5.234567
Epoch  0/30 | Step  10 | Loss: 2.345678
Epoch  0/30 | Step  20 | Loss: 1.234567
Epoch  0/30 | Step  30 | Loss: 0.654321

📈 Epoch  0 Summary: Avg Loss = 1.567890
   Current Parameters: W = 1.678901, b = 0.789012
   Parameter Errors: ΔW = 0.321099, Δb = 0.210988

...

📈 Epoch 29 Summary: Avg Loss = 0.000123
   Current Parameters: W = 1.999876, b = 1.000234
   Parameter Errors: ΔW = 0.000124, Δb = 0.000234

================================================================================
✅ Training Completed!
================================================================================

📊 Training Summary:
   - Initial Loss: 1.567890
   - Final Loss: 0.000123
   - Loss Reduction: 99.99%

🎯 Final Model Parameters:
   - Learned Weight: 1.999876
   - Learned Bias: 1.000234

🎓 Ground Truth Parameters:
   - True Weight: 2.000000
   - True Bias: 1.000000

📏 Parameter Estimation Errors:
   - Weight Error: 0.000124 (0.01%)
   - Bias Error: 0.000234 (0.02%)

🏆 Model Quality Assessment:
   ✨ Excellent! Parameters match ground truth within 1% error

📊 W&B Summary logged
   - View results at: https://wandb.ai/your-username/deepspeed-linear-regression/runs/abc123
   - W&B run finished successfully

================================================================================
🎉 Training Script Finished Successfully!
================================================================================
```

### GPU Monitoring

Watch GPU usage during training:

```bash
watch -n 0.1 nvidia-smi
```

### Training Metrics Tracked

The script automatically tracks and displays:

**During Training:**
- Step-by-step loss (every 10 steps)
- Epoch summaries (every 5 epochs)
- Current parameter estimates
- Parameter errors vs. ground truth

**Final Summary:**
- Total loss reduction
- Learned parameters vs. true parameters
- Absolute and percentage errors
- Quality assessment (Excellent/Good/Fair/Poor)

### Weights & Biases Dashboard

When W&B is enabled, you can view:

**Real-time Metrics:**
- `step_loss`: Loss at each logged step
- `epoch_avg_loss`: Average loss per epoch
- `learned_weight`: Weight parameter over time
- `learned_bias`: Bias parameter over time
- `weight_error`: Absolute error in weight
- `bias_error`: Absolute error in bias
- `weight_error_pct`: Percentage error in weight
- `bias_error_pct`: Percentage error in bias

**Final Summary:**
- Loss reduction percentage
- Final learned parameters
- Final parameter errors
- Quality score

**Project Info:**
- Project: `deepspeed-linear-regression`
- Run name: `simple-linear-model`
- Access at: https://wandb.ai/your-username/deepspeed-linear-regression

## Model Usage After Training

### Understanding the Trained Model

The model learns the linear relationship y = 2x + 1. After training:

```python
import torch
from train_ds import SimpleModel

# Load model
model = SimpleModel()
# Load trained weights if saved

# Test inference
x_test = torch.tensor([[5.0]])
y_pred = model(x_test)
print(f"Input: {x_test.item()}, Predicted: {y_pred.item()}, Expected: {2*5+1}")
```

## Troubleshooting

### Common Issues

#### CUDA Out of Memory
```json
// Reduce batch size in ds_config.json
{
  "train_batch_size": 16,
  "train_micro_batch_size_per_gpu": 16
}
```

#### DeepSpeed Installation Issues
```bash
# Install DeepSpeed with specific CUDA version
uv add "deepspeed>=0.12.0" --extra-index-url https://download.pytorch.org/whl/cu118

# Or build from source
DS_BUILD_OPS=1 uv add "deepspeed>=0.12.0"
```

#### FP16 Training Errors
```json
// Disable FP16 if your GPU doesn't support it
{
  "fp16": {
    "enabled": false
  }
}
```

#### Multi-GPU Training Not Working
```bash
# Check GPU availability
nvidia-smi

# Verify NCCL setup
export NCCL_DEBUG=INFO

# Test with single GPU first
uv run deepspeed --num_gpus=1 train_ds.py
```

#### Weights & Biases Issues

**W&B not installed:**
```bash
# Install wandb
uv add "wandb"

# Or with pip
pip install wandb
```

**W&B login issues:**
```bash
# Get your API key from https://wandb.ai/authorize
export WANDB_API_KEY="your_api_key_here"

# Verify it's set
echo $WANDB_API_KEY

# Or login interactively
wandb login
```

**Script works without W&B:**
The script is designed to never crash if W&B is not configured. You'll see one of these messages:

```
📊 Weights & Biases: Not installed
   - To enable tracking: pip install wandb
   - Then: export WANDB_API_KEY=your_api_key
```

Or:

```
📊 Weights & Biases: Not configured
   - To enable: export WANDB_API_KEY=your_api_key
   - To install: pip install wandb
```

Training will continue normally without W&B tracking.

### Performance Optimization

#### For Better Memory Usage
- Reduce `train_micro_batch_size_per_gpu`
- Increase `gradient_accumulation_steps`
- Disable `fp16` if not needed

#### For Faster Training
- Use multiple GPUs with DeepSpeed
- Enable `fp16` for compatible hardware
- Increase batch size if memory allows

## System Requirements

### Minimum Requirements
- **GPU**: 1x GTX 1080 Ti (11GB VRAM) or equivalent
- **RAM**: 8GB system RAM
- **Storage**: 5GB free space
- **CUDA**: 11.1+

### Recommended Setup
- **GPU**: 1x RTX 4090 (24GB) or A100
- **RAM**: 16GB system RAM
- **Storage**: 20GB SSD
- **Network**: Not required for single-node training

## Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make changes and test
4. Submit a pull request

## License

This project is licensed under the MIT License.

## Acknowledgments

- Microsoft for DeepSpeed optimization framework
- PyTorch team for the deep learning framework
- The open-source ML community

---

**Note**: This training example is designed for educational purposes to demonstrate DeepSpeed integration with minimal complexity. It's an ideal starting point for learning distributed training concepts.
