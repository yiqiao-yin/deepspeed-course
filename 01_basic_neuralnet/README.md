# Basic Neural Network with DeepSpeed

Train a simple linear regression model using DeepSpeed for distributed training on synthetic data.

## Features

- ðŸŽ¯ **Simple & Educational**: Perfect introduction to DeepSpeed with minimal complexity
- ðŸ“Š **Linear Regression**: Trains y = 2x + 1 model on synthetic data
- âš¡ **DeepSpeed Integration**: Demonstrates core DeepSpeed initialization and training loop
- ðŸ”§ **FP16 Training**: Mixed precision training for faster computation
- ðŸ’» **Multi-GPU Ready**: Supports distributed training across multiple GPUs

## Quick Start on RunPod

### 1. Initial Setup

Start with a fresh RunPod instance (recommend >= 1x RTX 4090 or A100):

```bash
# Install uv package manager
pip install uv

# Initialize new project
uv init basic-neuralnet-ds
cd basic-neuralnet-ds

# Add core dependencies
uv add "torch>=2.0.0"
uv add "deepspeed>=0.12.0"

# Development dependencies
uv add --dev "black" "isort" "flake8"
```

### 2. Project Structure

Create the following directory structure:

```
basic-neuralnet-ds/
â”œâ”€â”€ train_ds.py            # Your training script
â”œâ”€â”€ ds_config.json         # DeepSpeed configuration
â”œâ”€â”€ requirements.txt       # Generated by uv
â””â”€â”€ README.md             # This file
```

### 3. DeepSpeed Configuration

Create `ds_config.json`:

```json
{
  "train_batch_size": 32,
  "train_micro_batch_size_per_gpu": 32,
  "gradient_accumulation_steps": 1,
  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": 1e-3
    }
  },
  "fp16": {
    "enabled": true
  }
}
```

### 4. Add Your Training Script

Copy your `train_ds.py` script to the project directory.

## Running the Training

### Single GPU Training

```bash
uv run deepspeed --num_gpus=1 train_ds.py
```

### Multi-GPU Training with DeepSpeed

```bash
# For 2 GPUs
uv run deepspeed --num_gpus=2 train_ds.py

# For 4 GPUs
uv run deepspeed --num_gpus=4 train_ds.py

# For multi-node training
uv run deepspeed --num_gpus=8 --num_nodes=2 --node_rank=0 --master_addr="10.0.0.1" train_ds.py
```

### With Explicit Config File

```bash
uv run deepspeed --num_gpus=1 train_ds.py --deepspeed --deepspeed_config ds_config.json
```

## Configuration Options

### Model Settings

- **Model Architecture**: Simple Linear Layer (1 input â†’ 1 output)
- **Task**: Linear regression y = 2x + 1
- **Dataset**: Synthetic data (1000 samples, randomly generated)

### Training Hyperparameters

- **Learning Rate**: 1e-3
- **Optimizer**: Adam
- **Epochs**: 30
- **Batch Size**: 32 per device
- **Gradient Accumulation**: 1 step
- **Loss Function**: MSE (Mean Squared Error)

### Memory Optimization

- **Mixed Precision**: FP16
- **Train Batch Size**: 32
- **Micro Batch Size**: 32 per GPU

## Understanding the Training Script

The `train_ds.py` script demonstrates core DeepSpeed concepts:

### 1. Model Definition (train_ds.py:9-22)
```python
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(1, 1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.linear(x)
```

### 2. DeepSpeed Initialization (train_ds.py:42-46)
```python
model_engine, _, _, _ = deepspeed.initialize(
    model=model,
    model_parameters=model.parameters(),
    config="ds_config.json"
)
```

### 3. Training Loop (train_ds.py:53-65)
The training loop uses DeepSpeed's `backward()` and `step()` methods instead of standard PyTorch optimizer calls.

## Monitoring Training

Watch GPU usage during training:

```bash
watch -n 0.1 nvidia-smi
```

Expected output shows decreasing loss values:
```
Epoch 0 | Step 0 | Loss: 1.234567
Epoch 0 | Step 10 | Loss: 0.654321
Epoch 1 | Step 0 | Loss: 0.234567
...
```

## Model Usage After Training

### Understanding the Trained Model

The model learns the linear relationship y = 2x + 1. After training:

```python
import torch
from train_ds import SimpleModel

# Load model
model = SimpleModel()
# Load trained weights if saved

# Test inference
x_test = torch.tensor([[5.0]])
y_pred = model(x_test)
print(f"Input: {x_test.item()}, Predicted: {y_pred.item()}, Expected: {2*5+1}")
```

## Troubleshooting

### Common Issues

#### CUDA Out of Memory
```json
// Reduce batch size in ds_config.json
{
  "train_batch_size": 16,
  "train_micro_batch_size_per_gpu": 16
}
```

#### DeepSpeed Installation Issues
```bash
# Install DeepSpeed with specific CUDA version
uv add "deepspeed>=0.12.0" --extra-index-url https://download.pytorch.org/whl/cu118

# Or build from source
DS_BUILD_OPS=1 uv add "deepspeed>=0.12.0"
```

#### FP16 Training Errors
```json
// Disable FP16 if your GPU doesn't support it
{
  "fp16": {
    "enabled": false
  }
}
```

#### Multi-GPU Training Not Working
```bash
# Check GPU availability
nvidia-smi

# Verify NCCL setup
export NCCL_DEBUG=INFO

# Test with single GPU first
uv run deepspeed --num_gpus=1 train_ds.py
```

### Performance Optimization

#### For Better Memory Usage
- Reduce `train_micro_batch_size_per_gpu`
- Increase `gradient_accumulation_steps`
- Disable `fp16` if not needed

#### For Faster Training
- Use multiple GPUs with DeepSpeed
- Enable `fp16` for compatible hardware
- Increase batch size if memory allows

## System Requirements

### Minimum Requirements
- **GPU**: 1x GTX 1080 Ti (11GB VRAM) or equivalent
- **RAM**: 8GB system RAM
- **Storage**: 5GB free space
- **CUDA**: 11.1+

### Recommended Setup
- **GPU**: 1x RTX 4090 (24GB) or A100
- **RAM**: 16GB system RAM
- **Storage**: 20GB SSD
- **Network**: Not required for single-node training

## Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make changes and test
4. Submit a pull request

## License

This project is licensed under the MIT License.

## Acknowledgments

- Microsoft for DeepSpeed optimization framework
- PyTorch team for the deep learning framework
- The open-source ML community

---

**Note**: This training example is designed for educational purposes to demonstrate DeepSpeed integration with minimal complexity. It's an ideal starting point for learning distributed training concepts.
